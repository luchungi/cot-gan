{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 5855.988337133396\n",
      "            Iterations: 36\n",
      "            Function evaluations: 836\n",
      "            Gradient evaluations: 36\n",
      "                        Zero Mean - ARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:           gaussianized   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                       ARCH   Log-Likelihood:               -5855.99\n",
      "Distribution:                  Normal   AIC:                           11754.0\n",
      "Method:            Maximum Likelihood   BIC:                           11897.9\n",
      "                                        No. Observations:                 6999\n",
      "Date:                Mon, Apr 08 2024   Df Residuals:                     6999\n",
      "Time:                        09:26:51   Df Model:                            0\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega          0.0727  8.049e-03      9.032  1.686e-19  [5.693e-02,8.848e-02]\n",
      "alpha[1]       0.0186  1.285e-02      1.449      0.147 [-6.565e-03,4.380e-02]\n",
      "alpha[2]       0.0827  1.292e-02      6.401  1.547e-10    [5.736e-02,  0.108]\n",
      "alpha[3]       0.0258  1.273e-02      2.028  4.251e-02  [8.719e-04,5.075e-02]\n",
      "alpha[4]       0.0722  1.427e-02      5.061  4.163e-07    [4.426e-02,  0.100]\n",
      "alpha[5]       0.1752  1.634e-02     10.721  8.127e-27      [  0.143,  0.207]\n",
      "alpha[6]       0.0267  1.268e-02      2.101  3.562e-02  [1.793e-03,5.152e-02]\n",
      "alpha[7]       0.0282  1.428e-02      1.978  4.793e-02  [2.577e-04,5.622e-02]\n",
      "alpha[8]       0.0191  1.346e-02      1.419      0.156 [-7.277e-03,4.547e-02]\n",
      "alpha[9]       0.0556  1.477e-02      3.764  1.672e-04  [2.664e-02,8.452e-02]\n",
      "alpha[10]      0.1145  1.513e-02      7.571  3.713e-14    [8.487e-02,  0.144]\n",
      "alpha[11]      0.0196  1.386e-02      1.414      0.157 [-7.573e-03,4.676e-02]\n",
      "alpha[12]  4.4768e-03  1.334e-02      0.336      0.737 [-2.166e-02,3.061e-02]\n",
      "alpha[13]  2.5579e-11  1.147e-02  2.230e-09      1.000 [-2.248e-02,2.248e-02]\n",
      "alpha[14]      0.0266  1.424e-02      1.868  6.183e-02 [-1.316e-03,5.449e-02]\n",
      "alpha[15]      0.0318  1.366e-02      2.329  1.987e-02  [5.039e-03,5.858e-02]\n",
      "alpha[16]  2.4753e-11  1.425e-02  1.737e-09      1.000 [-2.793e-02,2.793e-02]\n",
      "alpha[17]  2.8329e-11  1.227e-02  2.309e-09      1.000 [-2.405e-02,2.405e-02]\n",
      "alpha[18]  3.9242e-11  1.414e-02  2.775e-09      1.000 [-2.771e-02,2.771e-02]\n",
      "alpha[19]      0.0806  1.513e-02      5.331  9.788e-08    [5.099e-02,  0.110]\n",
      "alpha[20]      0.0205  1.290e-02      1.588      0.112 [-4.793e-03,4.577e-02]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 09:26:51.977107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:51.998155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:51.998195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:51.999155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:51.999191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:51.999219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:52.268444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:52.268502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:52.268510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-08 09:26:52.268550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-08 09:26:52.268568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9725 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0b:00.0, compute capability: 8.6\n",
      "2024-04-08 09:26:52.967867: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "  0%|                                                                                                                         | 0/200 [00:00<?, ?it/s]2024-04-08 09:33:35.174446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-04-08 09:33:35.419191: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      " 50%|█████████████████████████████████████████████████▉                                                   | 99/200 [21:07<09:20,  5.55s/it, loss=1.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.0005 and disc_lr to 0.0005 at iteration 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████▏                                   | 129/200 [23:55<06:33,  5.54s/it, loss=2.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.00025 and disc_lr to 0.00025 at iteration 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████▏                       | 152/200 [26:04<04:32,  5.68s/it, loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.000125 and disc_lr to 0.000125 at iteration 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████▌       | 185/200 [29:07<01:24,  5.66s/it, loss=-2.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 6.25e-05 and disc_lr to 6.25e-05 at iteration 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [30:26<00:00,  9.13s/it, loss=-0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The entire training takes 30.474170088768005 minutes ---\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 5855.988337133396\n",
      "            Iterations: 36\n",
      "            Function evaluations: 836\n",
      "            Gradient evaluations: 36\n",
      "                        Zero Mean - ARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:           gaussianized   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                       ARCH   Log-Likelihood:               -5855.99\n",
      "Distribution:                  Normal   AIC:                           11754.0\n",
      "Method:            Maximum Likelihood   BIC:                           11897.9\n",
      "                                        No. Observations:                 6999\n",
      "Date:                Mon, Apr 08 2024   Df Residuals:                     6999\n",
      "Time:                        09:57:20   Df Model:                            0\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega          0.0727  8.049e-03      9.032  1.686e-19  [5.693e-02,8.848e-02]\n",
      "alpha[1]       0.0186  1.285e-02      1.449      0.147 [-6.565e-03,4.380e-02]\n",
      "alpha[2]       0.0827  1.292e-02      6.401  1.547e-10    [5.736e-02,  0.108]\n",
      "alpha[3]       0.0258  1.273e-02      2.028  4.251e-02  [8.719e-04,5.075e-02]\n",
      "alpha[4]       0.0722  1.427e-02      5.061  4.163e-07    [4.426e-02,  0.100]\n",
      "alpha[5]       0.1752  1.634e-02     10.721  8.127e-27      [  0.143,  0.207]\n",
      "alpha[6]       0.0267  1.268e-02      2.101  3.562e-02  [1.793e-03,5.152e-02]\n",
      "alpha[7]       0.0282  1.428e-02      1.978  4.793e-02  [2.577e-04,5.622e-02]\n",
      "alpha[8]       0.0191  1.346e-02      1.419      0.156 [-7.277e-03,4.547e-02]\n",
      "alpha[9]       0.0556  1.477e-02      3.764  1.672e-04  [2.664e-02,8.452e-02]\n",
      "alpha[10]      0.1145  1.513e-02      7.571  3.713e-14    [8.487e-02,  0.144]\n",
      "alpha[11]      0.0196  1.386e-02      1.414      0.157 [-7.573e-03,4.676e-02]\n",
      "alpha[12]  4.4768e-03  1.334e-02      0.336      0.737 [-2.166e-02,3.061e-02]\n",
      "alpha[13]  2.5579e-11  1.147e-02  2.230e-09      1.000 [-2.248e-02,2.248e-02]\n",
      "alpha[14]      0.0266  1.424e-02      1.868  6.183e-02 [-1.316e-03,5.449e-02]\n",
      "alpha[15]      0.0318  1.366e-02      2.329  1.987e-02  [5.039e-03,5.858e-02]\n",
      "alpha[16]  2.4753e-11  1.425e-02  1.737e-09      1.000 [-2.793e-02,2.793e-02]\n",
      "alpha[17]  2.8329e-11  1.227e-02  2.309e-09      1.000 [-2.405e-02,2.405e-02]\n",
      "alpha[18]  3.9242e-11  1.414e-02  2.775e-09      1.000 [-2.771e-02,2.771e-02]\n",
      "alpha[19]      0.0806  1.513e-02      5.331  9.788e-08    [5.099e-02,  0.110]\n",
      "alpha[20]      0.0205  1.290e-02      1.588      0.112 [-4.793e-03,4.577e-02]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                  | 99/200 [21:38<09:31,  5.66s/it, loss=-4.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.0005 and disc_lr to 0.0005 at iteration 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████▊                           | 145/200 [26:13<05:37,  6.14s/it, loss=-7.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.00025 and disc_lr to 0.00025 at iteration 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████      | 188/200 [30:44<01:19,  6.63s/it, loss=-7.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.000125 and disc_lr to 0.000125 at iteration 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [31:54<00:00,  9.57s/it, loss=-5.97]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The entire training takes 31.93025059700012 minutes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sinkhorn_eps_list = [1., 1.]\n",
    "reg_penalty_list = [0.1, 1.]\n",
    "\n",
    "assert len(sinkhorn_eps_list) == len(reg_penalty_list)\n",
    "\n",
    "for i in range(len(sinkhorn_eps_list)):\n",
    "    sinkhorn_eps = sinkhorn_eps_list[i]\n",
    "    reg_penalty = reg_penalty_list[i]\n",
    "\n",
    "    n_iters = 200\n",
    "    batch_size = 32\n",
    "    # sinkhorn_eps = 10. # entropy regularisation coefficent (will take log10 of this number and round to int)\n",
    "    sinkhorn_l = 200 # number of sinkhorn iterations\n",
    "    # reg_penalty = 0.1 # martingale regularisation penalty (will take log10 of this number and round to int)\n",
    "    gen_lr = 1e-3\n",
    "    disc_lr = 1e-3\n",
    "\n",
    "    gen_type = 'lstmd'\n",
    "    activation = 'tanh'\n",
    "    nlstm = 1\n",
    "    g_state_size = 64\n",
    "    d_state_size = 64``\n",
    "    log_series = True\n",
    "\n",
    "    dname = 'SPX'\n",
    "    seq_dim = 1 # dimension of the time series excluding time dimension\n",
    "    Dx = 2 # dimension of the time series including time dimension\n",
    "    time_steps = 250 # for the discriminator\n",
    "    sample_len = 300 # for the generator\n",
    "    hist_len = 50\n",
    "    stride = 50\n",
    "    seed = 42 # np.random.randint(0, 10000)\n",
    "    dt = 1 / 252\n",
    "\n",
    "    patience = 20\n",
    "    factor = 0.5\n",
    "    fig_freq = 10\n",
    "    training_params = {\n",
    "        'n_iters': n_iters,\n",
    "        'batch_size': batch_size,\n",
    "        'sinkhorn_eps': sinkhorn_eps,\n",
    "        'sinkhorn_l': sinkhorn_l,\n",
    "        'reg_penalty': reg_penalty,\n",
    "        'gen_lr': gen_lr,\n",
    "        'disc_lr': disc_lr,\n",
    "        'patience': patience,\n",
    "        'factor': factor,\n",
    "    }\n",
    "\n",
    "    model_params = {\n",
    "        'gen_type': gen_type,\n",
    "        'activation': activation,\n",
    "        # 'nlstm': nlstm,\n",
    "        'g_state_size': g_state_size,\n",
    "        'd_state_size': d_state_size,\n",
    "        'log_series': log_series,\n",
    "    }\n",
    "\n",
    "    data_params = {\n",
    "        'dname': dname,\n",
    "        'dt': dt,\n",
    "        'sample_len': sample_len,\n",
    "        'hist_len': hist_len,\n",
    "        'time_steps': time_steps,\n",
    "        'stride': stride,\n",
    "        'seed': seed,\n",
    "        'Dx': Dx,\n",
    "    }\n",
    "    parser = argparse.ArgumentParser(description='cot')\n",
    "    parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                        choices=['cot'])\n",
    "    parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "    parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "    parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "    parser.add_argument('-Dz', '--z_dims_t', type=int, default=4)\n",
    "    parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "                        help=\"batch norm\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "    test = args.test\n",
    "    bn = bool(args.bn)\n",
    "    g_output_activation = 'linear'\n",
    "\n",
    "    df = pd.read_csv('./data/spx_20231229.csv', index_col=0, parse_dates=True)\n",
    "    data_dist = data_utils.DFDataset(df, '1995-01-01', '2022-10-19', sample_len, batch_size, stride)\n",
    "    dataset = dname\n",
    "    n_layers = 1\n",
    "    gen_optimiser = tf.keras.optimizers.legacy.Adam(gen_lr)\n",
    "    dischm_optimiser = tf.keras.optimizers.legacy.Adam(disc_lr)\n",
    "    disc_iters = 1\n",
    "    scaling_coef = 1.0\n",
    "\n",
    "    # Define a standard multivariate normal for\n",
    "    # (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "    z_dims_t = args.z_dims_t\n",
    "    y_dims = args.Dy\n",
    "    dist_z = data_utils.GARCH(df, start_date='1995-01-01', end_date='2022-10-19', sample_len=300,\n",
    "                            p=20, o=0, q=0, mean_model='Zero', vol_model='GARCH', dist='gaussian',\n",
    "                            seed=42, stride=50)\n",
    "\n",
    "    # Create instances of generator, discriminator_h and\n",
    "    # discriminator_m CONV VERSION\n",
    "    g_filter_size = args.g_filter_size\n",
    "    d_filter_size = args.d_filter_size\n",
    "    disc_kernel_width = 5\n",
    "\n",
    "    generator = gan.GenLSTMd(z_dims_t, seq_dim, sample_len, hist_len, hidden_size=g_state_size)\n",
    "\n",
    "    discriminator_h = gan.ToyDiscriminator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "        kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "    discriminator_m = gan.ToyDiscriminator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "        kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "    if reg_penalty.is_integer() and sinkhorn_eps.is_integer():\n",
    "        suffix = f\"{dname[:3]}_e{int(sinkhorn_eps):d}r{int(reg_penalty):d}s{seed:d}\"\n",
    "    elif reg_penalty.is_integer() and not sinkhorn_eps.is_integer():\n",
    "        suffix = f\"{dname[:3]}_e{sinkhorn_eps:.3g}r{int(reg_penalty):d}s{seed:d}\"\n",
    "    elif not reg_penalty.is_integer() and sinkhorn_eps.is_integer():\n",
    "        suffix = f\"{dname[:3]}_e{int(sinkhorn_eps):d}r{reg_penalty:.3g}s{seed:d}\"\n",
    "    else:\n",
    "        suffix = f\"{dname[:3]}_e{sinkhorn_eps:.3g}r{reg_penalty:.3g}s{seed:d}\"\n",
    "\n",
    "    saved_file =  \"{}_{}{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                        datetime.now().strftime(\"%d\"),\n",
    "                                        datetime.now().strftime(\"%H\"),\n",
    "                                        datetime.now().strftime(\"%M\"),\n",
    "                                        datetime.now().strftime(\"%S\")) + suffix\n",
    "\n",
    "    log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "    # Create directories for storing images later.\n",
    "    if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "        os.makedirs(f\"trained/{saved_file}/data\")\n",
    "    if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "        os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "    # GAN train notes\n",
    "    with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "        # Include any experiment notes here:\n",
    "        f.write(\"Experiment notes: .... \\n\\n\")\n",
    "        f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "            dataset,\n",
    "            time_steps, ))\n",
    "        f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "            g_state_size,\n",
    "            n_layers,\n",
    "            reg_penalty))\n",
    "        f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "            batch_size,\n",
    "            disc_iters,\n",
    "            gen_lr,\n",
    "            disc_lr))\n",
    "        f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "            sinkhorn_eps,\n",
    "            sinkhorn_l))\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.text('training_params', data_utils.pretty_json(training_params), step=0)\n",
    "        tf.summary.text('model_params', data_utils.pretty_json(model_params), step=0)\n",
    "        tf.summary.text('data_params', data_utils.pretty_json(data_params), step=0)\n",
    "\n",
    "    @tf.function\n",
    "    def disc_training_step(real_data, real_data_p):\n",
    "        hidden_z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "        hidden_z_p = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as disc_tape:\n",
    "            fake_data = generator.call(hidden_z, real_data)         # For SPX\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p)   # For SPX\n",
    "\n",
    "            # h_fake = discriminator_h.call(fake_data)\n",
    "            # m_real = discriminator_m.call(real_data)\n",
    "            # m_fake = discriminator_m.call(fake_data)\n",
    "            # h_real_p = discriminator_h.call(real_data_p)\n",
    "            # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "            # m_real_p = discriminator_m.call(real_data_p)\n",
    "            # loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            #     h_real_p, h_fake_p)\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "            # NOTE: FOR USING hist_len ONWARDS FOR LOSS COMPUTATION\n",
    "            h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "            m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "            m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "            h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "            h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "            m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "            loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "                real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "                sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "                h_real_p, h_fake_p)\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "            pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "                m_real, reg_penalty, scaling_coef)\n",
    "            disc_loss = - loss1 + pm1\n",
    "        # update discriminator parameters\n",
    "        disch_grads, discm_grads = disc_tape.gradient(\n",
    "            disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "        dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "        dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def gen_training_step(real_data, real_data_p):\n",
    "        hidden_z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "        hidden_z_p = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake_data = generator.call(hidden_z, real_data)             # For SPX\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p)       # For SPX\n",
    "\n",
    "            # h and m networks used to compute the martingale penalty\n",
    "\n",
    "            # h_fake = discriminator_h.call(fake_data)\n",
    "            # m_real = discriminator_m.call(real_data)\n",
    "            # m_fake = discriminator_m.call(fake_data)\n",
    "            # h_real_p = discriminator_h.call(real_data_p)\n",
    "            # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "            # m_real_p = discriminator_m.call(real_data_p)\n",
    "            # loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            #     h_real_p, h_fake_p)\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "            # # NOTE: FOR USING hist_len ONWARDS FOR LOSS COMPUTATION\n",
    "            h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "            m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "            m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "            h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "            h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "            m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "            loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "                real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "                sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "                h_real_p, h_fake_p)\n",
    "\n",
    "    ############################################################################################################\n",
    "\n",
    "            gen_loss = loss2\n",
    "        # update generator parameters\n",
    "        generator_grads = gen_tape.gradient(\n",
    "            gen_loss, generator.trainable_variables)\n",
    "        gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "        return loss2\n",
    "\n",
    "    it_counts = 0\n",
    "    with tqdm.trange(n_iters, ncols=150) as it:\n",
    "        best_loss = [np.inf, 0]\n",
    "        for _ in it:\n",
    "            it_counts += 1\n",
    "            # generate a batch of REAL data\n",
    "            real_data = data_dist.batch(batch_size)\n",
    "            real_data_p = data_dist.batch(batch_size)\n",
    "            real_data = tf.cast(real_data, tf.float32)\n",
    "            real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "            disc_training_step(real_data, real_data_p)\n",
    "            loss = gen_training_step(real_data, real_data_p)\n",
    "            it.set_postfix(loss=float(loss))\n",
    "\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "                train_writer.flush()\n",
    "\n",
    "            if not np.isfinite(loss.numpy()):\n",
    "                # print('%s Loss exploded!' % model_fn)\n",
    "                print('Loss exploded')\n",
    "                # Open the existing file with mode a - append\n",
    "                with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                    # Include any experiment notes here:\n",
    "                    f.write(\"\\n Training failed! \")\n",
    "                break\n",
    "            else:\n",
    "                # check if the loss is the best so far and reduce lr if no improvement beyond patience\n",
    "                if loss < best_loss[0]:\n",
    "                    best_loss = [loss, it_counts]\n",
    "                if it_counts - best_loss[1] > patience:\n",
    "                    gen_lr *= factor\n",
    "                    disc_lr *= factor\n",
    "                    gen_optimiser.lr.assign(gen_lr)\n",
    "                    dischm_optimiser.lr.assign(disc_lr)\n",
    "                    best_loss = [loss, it_counts] # reset best loss iteration to current iteration for next patience\n",
    "                    print(f'Reducing gen_lr to {gen_lr} and disc_lr to {disc_lr} at iteration {it_counts}')\n",
    "\n",
    "                # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "                z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "                samples = generator.call(z, real_data, training=False) # For SPX\n",
    "\n",
    "                batch_series = np.asarray(samples[...,1])\n",
    "                if log_series:\n",
    "                    plt.plot(np.exp(batch_series.T))\n",
    "                    sample_mean = np.diff(batch_series, axis=1).mean() / dt\n",
    "                    sample_std = np.diff(batch_series, axis=1).std() / np.sqrt(dt)\n",
    "                else:\n",
    "                    plt.plot(batch_series.T)\n",
    "                    sample_mean = np.diff(np.log(batch_series), axis=1).mean() / dt\n",
    "                    sample_std = np.diff(np.log(batch_series), axis=1).std() / np.sqrt(dt)\n",
    "\n",
    "                with train_writer.as_default():\n",
    "                    if it_counts % fig_freq == 0:\n",
    "                        tf.summary.image(\"Generated samples\", data_utils.plot_to_image(plt.gcf()), step=it_counts)\n",
    "                    tf.summary.scalar('Stats/Sample_mean', sample_mean, step=it_counts)\n",
    "                    tf.summary.scalar('Stats/Sample_std', sample_std, step=it_counts)\n",
    "                # save model to file\n",
    "                generator.save_weights(f\"./trained/{saved_file}/generator/\")\n",
    "                discriminator_h.save_weights(f\"./trained/{saved_file}/discriminator_h/\")\n",
    "                discriminator_m.save_weights(f\"./trained/{saved_file}/discriminator_m/\")\n",
    "            continue\n",
    "\n",
    "    print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
