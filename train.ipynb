{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='cot')\n",
    "\n",
    "parser.add_argument('-d', '--dname', type=str, default='GBM',\n",
    "                    choices=['SineImage', 'AROne', 'eeg', 'GBM'])\n",
    "parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                    choices=['cot'])\n",
    "parser.add_argument('-s', '--seed', type=int, default=42)\n",
    "parser.add_argument('-gss', '--g_state_size', type=int, default=32)\n",
    "parser.add_argument('-dss', '--d_state_size', type=int, default=32)\n",
    "parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "parser.add_argument('-r', '--reg_penalty', type=float, default=10.0)\n",
    "parser.add_argument('-ts', '--time_steps', type=int, default=60)\n",
    "parser.add_argument('-sinke', '--sinkhorn_eps', type=float, default=100)\n",
    "parser.add_argument('-sinkl', '--sinkhorn_l', type=int, default=100)\n",
    "parser.add_argument('-Dx', '--Dx', type=int, default=1)\n",
    "parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "parser.add_argument('-Dz', '--z_dims_t', type=int, default=1)\n",
    "parser.add_argument('-g', '--gen', type=str, default=\"genlstm\",\n",
    "                    choices=[\"lstm\", \"fc\", \"genlstm\"])\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=64)\n",
    "parser.add_argument('-nlstm', '--nlstm', type=int, default=1,\n",
    "                    help=\"number of lstms in discriminator\")\n",
    "parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
    "parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "                    help=\"batch norm\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [03:54<00:00, 117.38s/it, loss=1.64]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The entire training takes 3.914323155085246 minutes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_iters = 2\n",
    "\n",
    "start_time = time.time()\n",
    "# hyper-parameter settings\n",
    "dname = args.dname\n",
    "test = args.test\n",
    "time_steps = args.time_steps\n",
    "batch_size = args.batch_size\n",
    "bn = bool(args.bn)\n",
    "if \"SLURM_ARRAY_TASK_ID\" in os.environ:\n",
    "    seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "else:\n",
    "    seed = args.seed\n",
    "\n",
    "Dx = args.Dx\n",
    "g_output_activation = 'linear'\n",
    "\n",
    "if dname == 'AROne':\n",
    "    data_dist = data_utils.AROne(\n",
    "        Dx, time_steps, np.linspace(0.1, 0.9, Dx), 0.5)\n",
    "elif dname == 'eeg':\n",
    "    data_dist = data_utils.EEGData(\n",
    "        Dx, time_steps, batch_size, n_iters, seed=seed)\n",
    "elif dname == 'SineImage':\n",
    "    data_dist = data_utils.SineImage(\n",
    "        length=time_steps, Dx=Dx, rand_std=0.1)\n",
    "elif dname == 'GBM':\n",
    "    data_dist = data_utils.GBM(mu=0.1, sigma=0.2, dt=1/252, length=time_steps, batch_size=batch_size, n_paths=batch_size*100, initial_value=1.0, time_dim=False)\n",
    "else:\n",
    "    ValueError('Data does not exist.')\n",
    "\n",
    "dataset = dname\n",
    "# Number of RNN layers stacked together\n",
    "n_layers = 1\n",
    "reg_penalty = args.reg_penalty\n",
    "gen_lr = args.lr\n",
    "disc_lr = args.lr\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# Add gradient clipping before updates\n",
    "gen_optimiser = tf.keras.optimizers.legacy.Adam(gen_lr)\n",
    "dischm_optimiser = tf.keras.optimizers.legacy.Adam(disc_lr)\n",
    "\n",
    "it_counts = 0\n",
    "disc_iters = 1\n",
    "sinkhorn_eps = args.sinkhorn_eps\n",
    "sinkhorn_l = args.sinkhorn_l\n",
    "nlstm = args.nlstm\n",
    "scaling_coef = 1.0\n",
    "\n",
    "# Define a standard multivariate normal for\n",
    "# (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "z_dims_t = args.z_dims_t\n",
    "y_dims = args.Dy\n",
    "# dist_z = tfp.distributions.Uniform(-1, 1)\n",
    "dist_z = tfp.distributions.Normal(0, 1)\n",
    "# dist_y = tfp.distributions.Uniform(-1, 1)\n",
    "\n",
    "# Create instances of generator, discriminator_h and\n",
    "# discriminator_m CONV VERSION\n",
    "g_state_size = args.g_state_size\n",
    "d_state_size = args.d_state_size\n",
    "g_filter_size = args.g_filter_size\n",
    "d_filter_size = args.d_filter_size\n",
    "disc_kernel_width = 5\n",
    "\n",
    "if args.gen == \"fc\":\n",
    "    generator = gan.SimpleGenerator(\n",
    "        batch_size, time_steps, Dx, g_filter_size, z_dims_t,\n",
    "        output_activation=g_output_activation)\n",
    "elif args.gen == \"lstm\":\n",
    "    generator = gan.ToyGenerator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, g_state_size, g_filter_size,\n",
    "        output_activation=g_output_activation, nlstm=nlstm, nlayer=2,\n",
    "        Dy=y_dims, bn=bn)\n",
    "elif args.gen == \"genlstm\":\n",
    "    generator = gan.GenLSTM(\n",
    "        z_dims_t, Dx, time_steps, hidden_size=g_state_size, n_lstm_layers=nlstm)\n",
    "\n",
    "discriminator_h = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "discriminator_m = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "# data_utils.check_model_summary(batch_size, z_dims, generator)\n",
    "# data_utils.check_model_summary(batch_size, seq_len, discriminator_h)\n",
    "\n",
    "lsinke = int(np.round(np.log10(sinkhorn_eps)))\n",
    "lreg = int(np.round(np.log10(reg_penalty)))\n",
    "saved_file = f\"{dname[:3]}_{test[0]}_e{lsinke:d}r{lreg:d}s{seed:d}\" + \\\n",
    "    \"{}_{}{}-{}-{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                    datetime.now().strftime(\"%d\"),\n",
    "                                    datetime.now().strftime(\"%H\"),\n",
    "                                    datetime.now().strftime(\"%M\"),\n",
    "                                    datetime.now().strftime(\"%S\"),\n",
    "                                    datetime.now().strftime(\"%f\"))\n",
    "\n",
    "model_fn = \"%s_Dz%d_Dy%d_Dx%d_bs%d_gss%d_gfs%d_dss%d_dfs%d_ts%d_r%d_eps%d_l%d_lr%d_nl%d_s%02d\" % (\n",
    "    dname, z_dims_t, y_dims, Dx, batch_size, g_state_size, g_filter_size,\n",
    "    d_state_size, d_filter_size, time_steps, np.round(np.log10(reg_penalty)),\n",
    "    np.round(np.log10(sinkhorn_eps)), sinkhorn_l, np.round(np.log10(args.lr)), nlstm, seed)\n",
    "\n",
    "log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "# Create directories for storing images later.\n",
    "if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/data\")\n",
    "if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "# GAN train notes\n",
    "with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "    # Include any experiment notes here:\n",
    "    f.write(\"Experiment notes: .... \\n\\n\")\n",
    "    f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "        dataset,\n",
    "        time_steps, ))\n",
    "    f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "        g_state_size,\n",
    "        n_layers,\n",
    "        reg_penalty))\n",
    "    f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "        batch_size,\n",
    "        disc_iters,\n",
    "        gen_lr,\n",
    "        disc_lr))\n",
    "    f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "        sinkhorn_eps,\n",
    "        sinkhorn_l))\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "with train_writer.as_default():\n",
    "    tf.summary.text('model_fn', model_fn, step=1)\n",
    "\n",
    "@tf.function\n",
    "def disc_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as disc_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        fake_data = generator.call(hidden_z)\n",
    "        fake_data_p = generator.call(hidden_z_p)\n",
    "\n",
    "        h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "        m_real = discriminator_m.call(real_data)\n",
    "        m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "        h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "        m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "        loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "            m_real, reg_penalty, scaling_coef)\n",
    "        disc_loss = - loss1 + pm1\n",
    "    # update discriminator parameters\n",
    "    disch_grads, discm_grads = disc_tape.gradient(\n",
    "        disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "    dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "    dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def gen_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        fake_data = generator.call(hidden_z)\n",
    "        fake_data_p = generator.call(hidden_z_p)\n",
    "\n",
    "        h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "        m_real = discriminator_m.call(real_data)\n",
    "        m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "        h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "        m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "        loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        gen_loss = loss2\n",
    "    # update generator parameters\n",
    "    generator_grads = gen_tape.gradient(\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "    return loss2\n",
    "\n",
    "with tqdm.trange(n_iters, ncols=150) as it:\n",
    "    for _ in it:\n",
    "        it_counts += 1\n",
    "        # generate a batch of REAL data\n",
    "        real_data = data_dist.batch(batch_size)\n",
    "        real_data_p = data_dist.batch(batch_size)\n",
    "        real_data = tf.cast(real_data, tf.float32)\n",
    "        real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "        disc_training_step(real_data, real_data_p)\n",
    "        loss = gen_training_step(real_data, real_data_p)\n",
    "        it.set_postfix(loss=float(loss))\n",
    "\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "            train_writer.flush()\n",
    "\n",
    "        if not np.isfinite(loss.numpy()):\n",
    "            print('%s Loss exploded!' % model_fn)\n",
    "            # Open the existing file with mode a - append\n",
    "            with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                # Include any experiment notes here:\n",
    "                f.write(\"\\n Training failed! \")\n",
    "            break\n",
    "        else:\n",
    "            if it_counts % 100 == 0 or it_counts <= 100:\n",
    "                # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "                z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "                # y = dist_y.sample([batch_size, y_dims])\n",
    "                # samples = generator.call(z, y, training=False)\n",
    "                samples = generator.call(z, training=False)\n",
    "                # save plot to file\n",
    "                if samples.shape[-1] == 1:\n",
    "                    data_utils.plot_batch(np.asarray(samples[..., 0]), it_counts, saved_file)\n",
    "\n",
    "                img = tf.transpose(tf.concat(list(samples), axis=1))[None, :, :, None]\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.image(\"Training data\", img, step=it_counts)\n",
    "                # save model to file\n",
    "                generator.save_weights(\"./trained/{}/{}/\".format(test,\n",
    "                                                                    model_fn))\n",
    "                discriminator_h.save_weights(\"./trained/{}/{}_h/\".format(test,\n",
    "                                                                            model_fn))\n",
    "                discriminator_m.save_weights(\"./trained/{}/{}_m/\".format(test,\n",
    "                                                                            model_fn))\n",
    "        continue\n",
    "\n",
    "print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
