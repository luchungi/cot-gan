{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    start_time = time.time()\n",
    "    # hyper-parameter settings\n",
    "    dname = args.dname\n",
    "    test = args.test\n",
    "    time_steps = args.time_steps\n",
    "    batch_size = args.batch_size\n",
    "    bn = bool(args.bn)\n",
    "    if \"SLURM_ARRAY_TASK_ID\" in os.environ:\n",
    "        seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "    else:\n",
    "        seed = args.seed\n",
    "\n",
    "    n_iters = 1000\n",
    "    Dx = 10\n",
    "    g_output_activation = 'linear'\n",
    "    time_steps = 100\n",
    "\n",
    "    if dname == 'AROne':\n",
    "        data_dist = data_utils.AROne(\n",
    "            Dx, time_steps, np.linspace(0.1, 0.9, Dx), 0.5)\n",
    "    elif dname == 'eeg':\n",
    "        data_dist = data_utils.EEGData(\n",
    "            Dx, time_steps, batch_size, n_iters, seed=seed)\n",
    "    elif dname == 'SineImage':\n",
    "        data_dist = data_utils.SineImage(\n",
    "            length=time_steps, Dx=Dx, rand_std=0.1)\n",
    "    elif dname == 'GBM':\n",
    "        data_dist = data_utils.GBM(mu=0.1, sigma=0.2, dt=1/252, length=60, batch_size=64, n_path=64*100, initial_value=1.0, time_dim=False)\n",
    "    else:\n",
    "        ValueError('Data does not exist.')\n",
    "\n",
    "    dataset = dname\n",
    "    # Number of RNN layers stacked together\n",
    "    n_layers = 1\n",
    "    reg_penalty = args.reg_penalty\n",
    "    gen_lr = args.lr\n",
    "    disc_lr = args.lr\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    # Add gradient clipping before updates\n",
    "    gen_optimiser = tf.keras.optimizers.Adam(gen_lr)\n",
    "    dischm_optimiser = tf.keras.optimizers.Adam(disc_lr)\n",
    "\n",
    "    it_counts = 0\n",
    "    disc_iters = 1\n",
    "    sinkhorn_eps = args.sinkhorn_eps\n",
    "    sinkhorn_l = args.sinkhorn_l\n",
    "    nlstm = args.nlstm\n",
    "    scaling_coef = 1.0\n",
    "\n",
    "    # Define a standard multivariate normal for\n",
    "    # (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "    z_dims_t = args.z_dims_t\n",
    "    y_dims = args.Dy\n",
    "    dist_z = tfp.distributions.Uniform(-1, 1)\n",
    "    dist_y = tfp.distributions.Uniform(-1, 1)\n",
    "\n",
    "    # Create instances of generator, discriminator_h and\n",
    "    # discriminator_m CONV VERSION\n",
    "    g_state_size = args.g_state_size\n",
    "    d_state_size = args.d_state_size\n",
    "    g_filter_size = args.g_filter_size\n",
    "    d_filter_size = args.d_filter_size\n",
    "    disc_kernel_width = 5\n",
    "\n",
    "    if args.gen == \"fc\":\n",
    "        generator = gan.SimpleGenerator(\n",
    "            batch_size, time_steps, Dx, g_filter_size, z_dims_t,\n",
    "            output_activation=g_output_activation)\n",
    "    elif args.gen == \"lstm\":\n",
    "        generator = gan.ToyGenerator(\n",
    "            batch_size, time_steps, z_dims_t, Dx, g_state_size, g_filter_size,\n",
    "            output_activation=g_output_activation, nlstm=nlstm, nlayer=2,\n",
    "            Dy=y_dims, bn=bn)\n",
    "\n",
    "    discriminator_h = gan.ToyDiscriminator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "        kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "    discriminator_m = gan.ToyDiscriminator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "        kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "    # data_utils.check_model_summary(batch_size, z_dims, generator)\n",
    "    # data_utils.check_model_summary(batch_size, seq_len, discriminator_h)\n",
    "\n",
    "    lsinke = int(np.round(np.log10(sinkhorn_eps)))\n",
    "    lreg = int(np.round(np.log10(reg_penalty)))\n",
    "    saved_file = f\"{dname[:3]}_{test[0]}_e{lsinke:d}r{lreg:d}s{seed:d}\" + \\\n",
    "        \"{}_{}{}-{}-{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                     datetime.now().strftime(\"%d\"),\n",
    "                                     datetime.now().strftime(\"%H\"),\n",
    "                                     datetime.now().strftime(\"%M\"),\n",
    "                                     datetime.now().strftime(\"%S\"),\n",
    "                                     datetime.now().strftime(\"%f\"))\n",
    "\n",
    "    model_fn = \"%s_Dz%d_Dy%d_Dx%d_bs%d_gss%d_gfs%d_dss%d_dfs%d_ts%d_r%d_eps%d_l%d_lr%d_nl%d_s%02d\" % (\n",
    "        dname, z_dims_t, y_dims, Dx, batch_size, g_state_size, g_filter_size,\n",
    "        d_state_size, d_filter_size, time_steps, np.round(np.log10(reg_penalty)),\n",
    "        np.round(np.log10(sinkhorn_eps)), sinkhorn_l, np.round(np.log10(args.lr)), nlstm, seed)\n",
    "\n",
    "    log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "    # Create directories for storing images later.\n",
    "    if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "        os.makedirs(f\"trained/{saved_file}/data\")\n",
    "    if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "        os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "    # GAN train notes\n",
    "    with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "        # Include any experiment notes here:\n",
    "        f.write(\"Experiment notes: .... \\n\\n\")\n",
    "        f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "            dataset,\n",
    "            time_steps, ))\n",
    "        f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "            g_state_size,\n",
    "            n_layers,\n",
    "            reg_penalty))\n",
    "        f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "            batch_size,\n",
    "            disc_iters,\n",
    "            gen_lr,\n",
    "            disc_lr))\n",
    "        f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "            sinkhorn_eps,\n",
    "            sinkhorn_l))\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.text('model_fn', model_fn, step=1)\n",
    "\n",
    "    @tf.function\n",
    "    def disc_training_step(real_data, real_data_p):\n",
    "        hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "        hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "        hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "        hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as disc_tape:\n",
    "            fake_data = generator.call(hidden_z, hidden_y)\n",
    "            fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "\n",
    "            h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "            m_real = discriminator_m.call(real_data)\n",
    "            m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "            h_real_p = discriminator_h.call(real_data_p)\n",
    "            h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "            m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "            loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "                real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "                sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "                h_real_p, h_fake_p)\n",
    "            pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "                m_real, reg_penalty, scaling_coef)\n",
    "            disc_loss = - loss1 + pm1\n",
    "        # update discriminator parameters\n",
    "        disch_grads, discm_grads = disc_tape.gradient(\n",
    "            disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "        dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "        dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def gen_training_step(real_data, real_data_p):\n",
    "        hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "        hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "        hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "        hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake_data = generator.call(hidden_z, hidden_y)\n",
    "            fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "\n",
    "            h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "            m_real = discriminator_m.call(real_data)\n",
    "            m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "            h_real_p = discriminator_h.call(real_data_p)\n",
    "            h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "            m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "            loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "                real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "                sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "                h_real_p, h_fake_p)\n",
    "            gen_loss = loss2\n",
    "        # update generator parameters\n",
    "        generator_grads = gen_tape.gradient(\n",
    "            gen_loss, generator.trainable_variables)\n",
    "        gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "        return loss2\n",
    "\n",
    "    with tqdm.trange(n_iters, ncols=150) as it:\n",
    "        for _ in it:\n",
    "            it_counts += 1\n",
    "            # generate a batch of REAL data\n",
    "            real_data = data_dist.batch(batch_size)\n",
    "            real_data_p = data_dist.batch(batch_size)\n",
    "            real_data = tf.cast(real_data, tf.float32)\n",
    "            real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "            disc_training_step(real_data, real_data_p)\n",
    "            loss = gen_training_step(real_data, real_data_p)\n",
    "            it.set_postfix(loss=float(loss))\n",
    "\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "                train_writer.flush()\n",
    "\n",
    "            if not np.isfinite(loss.numpy()):\n",
    "                print('%s Loss exploded!' % model_fn)\n",
    "                # Open the existing file with mode a - append\n",
    "                with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                    # Include any experiment notes here:\n",
    "                    f.write(\"\\n Training failed! \")\n",
    "                break\n",
    "            else:\n",
    "                if it_counts % 100 == 0 or it_counts <= 10:\n",
    "                    # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "                    z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "                    y = dist_y.sample([batch_size, y_dims])\n",
    "                    samples = generator.call(z, y, training=False)\n",
    "                    # save plot to file\n",
    "                    if samples.shape[-1] == 1:\n",
    "                        data_utils.plot_batch(np.asarray(samples[..., 0]), it_counts, saved_file)\n",
    "\n",
    "                    img = tf.transpose(tf.concat(list(samples[:5]), axis=1))[None, :, :, None]\n",
    "                    with train_writer.as_default():\n",
    "                        tf.summary.image(\"Training data\", img, step=it_counts)\n",
    "                    # save model to file\n",
    "                    generator.save_weights(\"./trained/{}/{}/\".format(test,\n",
    "                                                                     model_fn))\n",
    "                    discriminator_h.save_weights(\"./trained/{}/{}_h/\".format(test,\n",
    "                                                                             model_fn))\n",
    "                    discriminator_m.save_weights(\"./trained/{}/{}_m/\".format(test,\n",
    "                                                                             model_fn))\n",
    "            continue\n",
    "\n",
    "    print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "  0%|                                                                                                                        | 0/1000 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"/var/folders/nd/wwb7zyc94wsf4ql14n77712h0000gn/T/ipykernel_31068/701606213.py\", line 167, in disc_training_step  *\n        dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients  **\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable conv1d_2/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/luchungi/Google Drive/Programming/Git/cot-gan/train.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m-bn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m--bn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m, default\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                     help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch norm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m args, unknown \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_known_args()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train(args)\n",
      "\u001b[1;32m/Users/luchungi/Google Drive/Programming/Git/cot-gan/train.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m real_data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(real_data, tf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m real_data_p \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(real_data_p, tf\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m disc_training_step(real_data, real_data_p)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m loss \u001b[39m=\u001b[39m gen_training_step(real_data, real_data_p)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/luchungi/Google%20Drive/Programming/Git/cot-gan/train.ipynb#W2sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m it\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(loss))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/nd/wwb7zyc94wsf4ql14n77712h0000gn/T/__autograph_generated_fileov3m1n8y.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__disc_training_step\u001b[0;34m(real_data, real_data_p)\u001b[0m\n\u001b[1;32m     38\u001b[0m disch_grads, discm_grads \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(disc_tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(disc_loss), [ag__\u001b[39m.\u001b[39mld(discriminator_h)\u001b[39m.\u001b[39mtrainable_variables, ag__\u001b[39m.\u001b[39mld(discriminator_m)\u001b[39m.\u001b[39mtrainable_variables]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     39\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(dischm_optimiser)\u001b[39m.\u001b[39mapply_gradients, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(disch_grads), ag__\u001b[39m.\u001b[39mld(discriminator_h)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 40\u001b[0m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(dischm_optimiser)\u001b[39m.\u001b[39;49mapply_gradients, (ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mzip\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(discm_grads), ag__\u001b[39m.\u001b[39;49mld(discriminator_m)\u001b[39m.\u001b[39;49mtrainable_variables), \u001b[39mNone\u001b[39;49;00m, fscope),), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:1223\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1222\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1223\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:1253\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1253\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[1;32m   1254\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[1;32m   1255\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[1;32m   1256\u001b[0m     grads_and_vars,\n\u001b[1;32m   1257\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:1345\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1344\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1345\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m   1346\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1347\u001b[0m     )\n\u001b[1;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[1;32m   1350\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:1342\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1341\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py:233\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m--> 233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mupdate different parts of the model separately. Please call \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`optimizer.build(variables)` with the full list of trainable \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvariables before the training loop or use legacy optimizer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_step(gradient, variable)\n",
      "\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/var/folders/nd/wwb7zyc94wsf4ql14n77712h0000gn/T/ipykernel_31068/701606213.py\", line 167, in disc_training_step  *\n        dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1223, in apply_gradients  **\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 652, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1253, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1345, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 1342, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/Users/luchungi/opt/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 233, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable conv1d_2/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='cot')\n",
    "\n",
    "parser.add_argument('-d', '--dname', type=str, default='SineImage',\n",
    "                    choices=['SineImage', 'AROne', 'eeg'])\n",
    "parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                    choices=['cot'])\n",
    "parser.add_argument('-s', '--seed', type=int, default=42)\n",
    "parser.add_argument('-gss', '--g_state_size', type=int, default=32)\n",
    "parser.add_argument('-dss', '--d_state_size', type=int, default=32)\n",
    "parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "parser.add_argument('-r', '--reg_penalty', type=float, default=10.0)\n",
    "parser.add_argument('-ts', '--time_steps', type=int, default=48)\n",
    "parser.add_argument('-sinke', '--sinkhorn_eps', type=float, default=100)\n",
    "parser.add_argument('-sinkl', '--sinkhorn_l', type=int, default=100)\n",
    "parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "parser.add_argument('-Dz', '--z_dims_t', type=int, default=10)\n",
    "parser.add_argument('-g', '--gen', type=str, default=\"lstm\",\n",
    "                    choices=[\"lstm\", \"fc\"])\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=32)\n",
    "parser.add_argument('-nlstm', '--nlstm', type=int, default=1,\n",
    "                    help=\"number of lstms in discriminator\")\n",
    "parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
    "parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "                    help=\"batch norm\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
