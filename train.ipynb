{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "batch_size = 64\n",
    "sinkhorn_eps = 100 # entropy regularisation coefficent (will take log10 of this number and round to int)\n",
    "sinkhorn_l = 100 # number of sinkhorn iterations\n",
    "reg_penalty = 100 # martingale regularisation penalty (will take log10 of this number and round to int)\n",
    "lr = 1e-3\n",
    "\n",
    "gen_type = 'lstmpdt'\n",
    "nlstm = 1\n",
    "g_state_size = 64\n",
    "d_state_size = 64\n",
    "bn = True\n",
    "log_series = True\n",
    "\n",
    "dname = 'OU'\n",
    "time_steps = 60\n",
    "dt = 1.0 / 252\n",
    "seed = 42 # np.random.randint(0, 10000)\n",
    "Dx = 1 # dimension of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_iters': n_iters,\n",
    "    'batch_size': batch_size,\n",
    "    'sinkhorn_eps': sinkhorn_eps,\n",
    "    'sinkhorn_l': sinkhorn_l,\n",
    "    'reg_penalty': reg_penalty,\n",
    "    'lr': lr,\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'gen_type': gen_type,\n",
    "    'nlstm': nlstm,\n",
    "    'g_state_size': g_state_size,\n",
    "    'd_state_size': d_state_size,\n",
    "    'bn': bn,\n",
    "    'log_series': log_series,\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    'dname': dname,\n",
    "    'dt': dt,\n",
    "    'time_steps': time_steps,\n",
    "    'seed': seed,\n",
    "    'Dx': Dx,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='cot')\n",
    "\n",
    "# parser.add_argument('-d', '--dname', type=str, default='GBM',\n",
    "#                     choices=['SineImage', 'AROne', 'eeg', 'GBM'])\n",
    "parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                    choices=['cot'])\n",
    "# parser.add_argument('-s', '--seed', type=int, default=42)\n",
    "# parser.add_argument('-gss', '--g_state_size', type=int, default=32)\n",
    "# parser.add_argument('-dss', '--d_state_size', type=int, default=32)\n",
    "parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "# parser.add_argument('-r', '--reg_penalty', type=float, default=10.0) # martingale regularisation coefficent\n",
    "# parser.add_argument('-ts', '--time_steps', type=int, default=60)\n",
    "# parser.add_argument('-sinke', '--sinkhorn_eps', type=float, default=100) # entropy regularisation coefficent\n",
    "# parser.add_argument('-sinkl', '--sinkhorn_l', type=int, default=100) # number of sinkhorn iterations\n",
    "# parser.add_argument('-Dx', '--Dx', type=int, default=1)\n",
    "parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "parser.add_argument('-Dz', '--z_dims_t', type=int, default=1)\n",
    "# parser.add_argument('-g', '--gen', type=str, default=\"genlstm\",\n",
    "#                     choices=[\"lstm\", \"fc\", \"genlstm\"])\n",
    "# parser.add_argument('-bs', '--batch_size', type=int, default=64)\n",
    "# parser.add_argument('-nlstm', '--nlstm', type=int, default=1,\n",
    "                    # help=\"number of lstms in discriminator\")\n",
    "# parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
    "# parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "#                     help=\"batch norm\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 16:26:51.136882: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-27 16:26:51.136906: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-27 16:26:51.136911: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-27 16:26:51.136945: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-27 16:26:51.136961: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "  0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]2023-11-27 16:28:12.100201: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [50:30<00:00, 30.30s/it, loss=0.0193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The entire training takes 50.507206706206006 minutes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# hyper-parameter settings\n",
    "# dname = args.dname\n",
    "test = args.test\n",
    "# time_steps = args.time_steps\n",
    "# batch_size = args.batch_size\n",
    "# bn = bool(args.bn)\n",
    "# if \"SLURM_ARRAY_TASK_ID\" in os.environ:\n",
    "#     seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "# else:\n",
    "#     seed = args.seed\n",
    "\n",
    "# Dx = args.Dx\n",
    "g_output_activation = 'linear'\n",
    "\n",
    "if dname == 'AROne':\n",
    "    data_dist = data_utils.AROne(\n",
    "        Dx, time_steps, np.linspace(0.1, 0.9, Dx), 0.5)\n",
    "elif dname == 'eeg':\n",
    "    data_dist = data_utils.EEGData(\n",
    "        Dx, time_steps, batch_size, n_iters, seed=seed)\n",
    "elif dname == 'SineImage':\n",
    "    data_dist = data_utils.SineImage(\n",
    "        length=time_steps, Dx=Dx, rand_std=0.1)\n",
    "elif dname == 'GBM':\n",
    "    data_dist = data_utils.GBM(mu=0.1, sigma=0.2, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                               log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'OU':\n",
    "    data_dist = data_utils.OU(kappa=10., theta=1., sigma=0.5, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                              log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'Heston':\n",
    "    data_dist = data_utils.Heston(mu=0.2, v0=0.25, kappa=1., theta=0.16, rho=-0.7, sigma=0.2, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                                  log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "else:\n",
    "    ValueError('Data does not exist.')\n",
    "\n",
    "dataset = dname\n",
    "# Number of RNN layers stacked together\n",
    "n_layers = 1\n",
    "# reg_penalty = args.reg_penalty\n",
    "# gen_lr = args.lr\n",
    "# disc_lr = args.lr\n",
    "gen_lr = lr\n",
    "disc_lr = lr\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# Add gradient clipping before updates\n",
    "gen_optimiser = tf.keras.optimizers.legacy.Adam(gen_lr)\n",
    "dischm_optimiser = tf.keras.optimizers.legacy.Adam(disc_lr)\n",
    "\n",
    "it_counts = 0\n",
    "disc_iters = 1\n",
    "# sinkhorn_eps = args.sinkhorn_eps\n",
    "# sinkhorn_l = args.sinkhorn_l\n",
    "# nlstm = args.nlstm\n",
    "scaling_coef = 1.0\n",
    "\n",
    "# Define a standard multivariate normal for\n",
    "# (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "z_dims_t = args.z_dims_t\n",
    "y_dims = args.Dy\n",
    "# dist_z = tfp.distributions.Uniform(-1, 1)\n",
    "dist_z = tfp.distributions.Normal(0, 1)\n",
    "# dist_y = tfp.distributions.Uniform(-1, 1)\n",
    "\n",
    "# Create instances of generator, discriminator_h and\n",
    "# discriminator_m CONV VERSION\n",
    "# g_state_size = args.g_state_size\n",
    "# d_state_size = args.d_state_size\n",
    "g_filter_size = args.g_filter_size\n",
    "d_filter_size = args.d_filter_size\n",
    "disc_kernel_width = 5\n",
    "\n",
    "if gen_type == \"fc\":\n",
    "    generator = gan.SimpleGenerator(\n",
    "        batch_size, time_steps, Dx, g_filter_size, z_dims_t,\n",
    "        output_activation=g_output_activation)\n",
    "elif gen_type == \"lstm\":\n",
    "    generator = gan.ToyGenerator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, g_state_size, g_filter_size,\n",
    "        output_activation=g_output_activation, nlstm=nlstm, nlayer=2,\n",
    "        Dy=y_dims, bn=bn)\n",
    "elif gen_type == \"genlstm\":\n",
    "    generator = gan.GenLSTM(z_dims_t, Dx, time_steps, hidden_size=g_state_size, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmp\":\n",
    "    generator = gan.GenLSTMp(z_dims_t, Dx, time_steps, hidden_size=g_state_size, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmpdt\":\n",
    "    generator = gan.GenLSTMpdt(z_dims_t, Dx, time_steps, dt, hidden_size=g_state_size, n_lstm_layers=nlstm, log_series=log_series)\n",
    "\n",
    "discriminator_h = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "discriminator_m = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "# data_utils.check_model_summary(batch_size, z_dims, generator)\n",
    "# data_utils.check_model_summary(batch_size, seq_len, discriminator_h)\n",
    "\n",
    "lsinke = int(np.round(np.log10(sinkhorn_eps)))\n",
    "lreg = int(np.round(np.log10(reg_penalty)))\n",
    "saved_file = f\"{dname[:3]}_{test[0]}_e{lsinke:d}r{lreg:d}s{seed:d}\" + \\\n",
    "    \"{}_{}{}-{}-{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                    datetime.now().strftime(\"%d\"),\n",
    "                                    datetime.now().strftime(\"%H\"),\n",
    "                                    datetime.now().strftime(\"%M\"),\n",
    "                                    datetime.now().strftime(\"%S\"),\n",
    "                                    datetime.now().strftime(\"%f\"))\n",
    "\n",
    "model_fn = \"%s_Dz%d_Dy%d_Dx%d_bs%d_gss%d_gfs%d_dss%d_dfs%d_ts%d_r%d_eps%d_l%d_lr%d_nl%d_s%02d\" % (\n",
    "    dname, z_dims_t, y_dims, Dx, batch_size, g_state_size, g_filter_size,\n",
    "    d_state_size, d_filter_size, time_steps, np.round(np.log10(reg_penalty)),\n",
    "    np.round(np.log10(sinkhorn_eps)), sinkhorn_l, np.round(np.log10(lr)), nlstm, seed)\n",
    "\n",
    "log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "# Create directories for storing images later.\n",
    "if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/data\")\n",
    "if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "# GAN train notes\n",
    "with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "    # Include any experiment notes here:\n",
    "    f.write(\"Experiment notes: .... \\n\\n\")\n",
    "    f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "        dataset,\n",
    "        time_steps, ))\n",
    "    f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "        g_state_size,\n",
    "        n_layers,\n",
    "        reg_penalty))\n",
    "    f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "        batch_size,\n",
    "        disc_iters,\n",
    "        gen_lr,\n",
    "        disc_lr))\n",
    "    f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "        sinkhorn_eps,\n",
    "        sinkhorn_l))\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "with train_writer.as_default():\n",
    "    tf.summary.text('training_params', data_utils.pretty_json(training_params), step=0)\n",
    "    tf.summary.text('model_params', data_utils.pretty_json(model_params), step=0)\n",
    "    tf.summary.text('data_params', data_utils.pretty_json(data_params), step=0)\n",
    "\n",
    "@tf.function\n",
    "def disc_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as disc_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        fake_data = generator.call(hidden_z)\n",
    "        fake_data_p = generator.call(hidden_z_p)\n",
    "\n",
    "        h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "        m_real = discriminator_m.call(real_data)\n",
    "        m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "        h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "        m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "        loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "            m_real, reg_penalty, scaling_coef)\n",
    "        disc_loss = - loss1 + pm1\n",
    "    # update discriminator parameters\n",
    "    disch_grads, discm_grads = disc_tape.gradient(\n",
    "        disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "    dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "    dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def gen_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        fake_data = generator.call(hidden_z)\n",
    "        fake_data_p = generator.call(hidden_z_p)\n",
    "\n",
    "        # h and m networks used to compute the martingale penalty\n",
    "\n",
    "        h_fake = discriminator_h.call(fake_data)\n",
    "\n",
    "        m_real = discriminator_m.call(real_data)\n",
    "        m_fake = discriminator_m.call(fake_data)\n",
    "\n",
    "        h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p)\n",
    "\n",
    "        m_real_p = discriminator_m.call(real_data_p)\n",
    "\n",
    "        loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        gen_loss = loss2\n",
    "    # update generator parameters\n",
    "    generator_grads = gen_tape.gradient(\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "    return loss2\n",
    "\n",
    "with tqdm.trange(n_iters, ncols=150) as it:\n",
    "    for _ in it:\n",
    "        it_counts += 1\n",
    "        # generate a batch of REAL data\n",
    "        real_data = data_dist.batch(batch_size)\n",
    "        real_data_p = data_dist.batch(batch_size)\n",
    "        real_data = tf.cast(real_data, tf.float32)\n",
    "        real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "        disc_training_step(real_data, real_data_p)\n",
    "        loss = gen_training_step(real_data, real_data_p)\n",
    "        it.set_postfix(loss=float(loss))\n",
    "\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "            train_writer.flush()\n",
    "\n",
    "        if not np.isfinite(loss.numpy()):\n",
    "            print('%s Loss exploded!' % model_fn)\n",
    "            # Open the existing file with mode a - append\n",
    "            with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                # Include any experiment notes here:\n",
    "                f.write(\"\\n Training failed! \")\n",
    "            break\n",
    "        else:\n",
    "            # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "            z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "            # y = dist_y.sample([batch_size, y_dims])\n",
    "            # samples = generator.call(z, y, training=False)\n",
    "            samples = generator.call(z, training=False)\n",
    "            batch_series = np.asarray(samples[..., 0])\n",
    "            if log_series:\n",
    "                plt.plot(np.exp(batch_series.T))\n",
    "                sample_mean = np.diff(batch_series, axis=1).mean() / dt\n",
    "                sample_std = np.diff(batch_series, axis=1).std() / np.sqrt(dt)\n",
    "            else:\n",
    "                plt.plot(batch_series.T)\n",
    "                sample_mean = np.diff(np.log(batch_series), axis=1).mean() / dt\n",
    "                sample_std = np.diff(np.log(batch_series), axis=1).std() / np.sqrt(dt)\n",
    "            # save plot to file\n",
    "            # if samples.shape[-1] == 1:\n",
    "            #     data_utils.plot_batch(np.asarray(samples[..., 0]), it_counts, saved_file)\n",
    "\n",
    "            # img = tf.transpose(tf.concat(list(samples), axis=1))[None, :, :, None]\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.image(\"Generated samples\", data_utils.plot_to_image(plt.gcf()), step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_mean', sample_mean, step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_std', sample_std, step=it_counts)\n",
    "            # save model to file\n",
    "            generator.save_weights(f\"./trained/{saved_file}/generator/\")\n",
    "            discriminator_h.save_weights(f\"./trained/{saved_file}/discriminator_h/\")\n",
    "            discriminator_m.save_weights(f\"./trained/{saved_file}/discriminator_m/\")\n",
    "        continue\n",
    "\n",
    "print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
