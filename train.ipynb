{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 13:56:03.372613: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-12 13:56:03.372660: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-12 13:56:03.372683: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-12 13:56:03.377344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_iters = 500\n",
    "batch_size = 32\n",
    "sinkhorn_eps = 1. # entropy regularisation coefficent\n",
    "sinkhorn_l = 200 # number of sinkhorn iterations\n",
    "reg_penalty = 1. # martingale regularisation penalty\n",
    "gen_lr = 1e-3\n",
    "disc_lr = 1e-3\n",
    "\n",
    "gen_type = 'music'\n",
    "activation = 'tanh'\n",
    "nlstm = 1\n",
    "g_state_size = 64\n",
    "d_state_size = 64\n",
    "log_series = True\n",
    "\n",
    "dname = 'Music'\n",
    "z_dims_t = 1\n",
    "seq_dim = 1 # dimension of the time series excluding time dimension\n",
    "Dx = 3 # dimension of the time series including time dimension\n",
    "time_steps = 20 # for the discriminator\n",
    "sample_len = 30 # for the generator\n",
    "hist_len = 10\n",
    "stride = 800\n",
    "seed = 42 # np.random.randint(0, 10000)\n",
    "dt = 1 / 252\n",
    "\n",
    "patience = 20\n",
    "factor = 0.5\n",
    "fig_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_iters': n_iters,\n",
    "    'batch_size': batch_size,\n",
    "    'sinkhorn_eps': sinkhorn_eps,\n",
    "    'sinkhorn_l': sinkhorn_l,\n",
    "    'reg_penalty': reg_penalty,\n",
    "    'gen_lr': gen_lr,\n",
    "    'disc_lr': disc_lr,\n",
    "    'patience': patience,\n",
    "    'factor': factor,\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'gen_type': gen_type,\n",
    "    'activation': activation,\n",
    "    # 'nlstm': nlstm,\n",
    "    'z_dims_t': z_dims_t,\n",
    "    'g_state_size': g_state_size,\n",
    "    'd_state_size': d_state_size,\n",
    "    'log_series': log_series,\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    'dname': dname,\n",
    "    'dt': dt,\n",
    "    'sample_len': sample_len,\n",
    "    'hist_len': hist_len,\n",
    "    'time_steps': time_steps,\n",
    "    'stride': stride,\n",
    "    'seed': seed,\n",
    "    'Dx': Dx,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='cot')\n",
    "\n",
    "# parser.add_argument('-d', '--dname', type=str, default='GBM',\n",
    "#                     choices=['SineImage', 'AROne', 'eeg', 'GBM'])\n",
    "parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                    choices=['cot'])\n",
    "# parser.add_argument('-s', '--seed', type=int, default=42)\n",
    "# parser.add_argument('-gss', '--g_state_size', type=int, default=32)\n",
    "# parser.add_argument('-dss', '--d_state_size', type=int, default=32)\n",
    "parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "# parser.add_argument('-r', '--reg_penalty', type=float, default=10.0) # martingale regularisation coefficent\n",
    "# parser.add_argument('-ts', '--time_steps', type=int, default=60)\n",
    "# parser.add_argument('-sinke', '--sinkhorn_eps', type=float, default=100) # entropy regularisation coefficent\n",
    "# parser.add_argument('-sinkl', '--sinkhorn_l', type=int, default=100) # number of sinkhorn iterations\n",
    "# parser.add_argument('-Dx', '--Dx', type=int, default=1)\n",
    "parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "# parser.add_argument('-Dz', '--z_dims_t', type=int, default=4)\n",
    "# parser.add_argument('-g', '--gen', type=str, default=\"genlstm\",\n",
    "#                     choices=[\"lstm\", \"fc\", \"genlstm\"])\n",
    "# parser.add_argument('-bs', '--batch_size', type=int, default=38)\n",
    "# parser.add_argument('-nlstm', '--nlstm', type=int, default=1,\n",
    "                    # help=\"number of lstms in discriminator\")\n",
    "# parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
    "parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "                    help=\"batch norm\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 13:56:08.053167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.074089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.074140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.075348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.075391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.075424: I tensorflow/compile"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.297326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.297384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.297392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-12 13:56:08.297434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-12 13:56:08.297452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9725 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0b:00.0, compute capability: 8.6\n",
      "2024-04-12 13:56:08.694592: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "  0%|                                                                                                                         | 0/500 [00:00<?, ?it/s]2024-04-12 13:56:19.412114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2024-04-12 13:56:19.487084: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      " 19%|██████████████████▊                                                                               | 96/500 [07:45<31:00,  4.60s/it, loss=1.87e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.0005 and disc_lr to 0.0005 at iteration 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████████▉                                                                       | 134/500 [10:40<27:46,  4.55s/it, loss=1.82e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.00025 and disc_lr to 0.00025 at iteration 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████▎                                                               | 172/500 [13:37<25:48,  4.72s/it, loss=1.65e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 0.000125 and disc_lr to 0.000125 at iteration 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████▍                                                         | 203/500 [16:01<23:09,  4.68s/it, loss=1.81e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 6.25e-05 and disc_lr to 6.25e-05 at iteration 203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|███████████████████████████████████████████████▎                                                 | 244/500 [19:11<19:40,  4.61s/it, loss=1.89e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 3.125e-05 and disc_lr to 3.125e-05 at iteration 244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████▊                                            | 272/500 [21:19<17:40,  4.65s/it, loss=1.86e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 1.5625e-05 and disc_lr to 1.5625e-05 at iteration 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████████▌                                       | 297/500 [23:15<15:35,  4.61s/it, loss=1.69e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 7.8125e-06 and disc_lr to 7.8125e-06 at iteration 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|█████████████████████████████████████████████████████████████████████████▎                       | 378/500 [29:20<09:08,  4.50s/it, loss=1.42e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 3.90625e-06 and disc_lr to 3.90625e-06 at iteration 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████████████████████████▍                  | 404/500 [31:19<07:16,  4.54s/it, loss=1.65e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 1.953125e-06 and disc_lr to 1.953125e-06 at iteration 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|███████████████████████████████████████████████████████████████████████████████████▏             | 429/500 [33:13<05:23,  4.55s/it, loss=1.45e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 9.765625e-07 and disc_lr to 9.765625e-07 at iteration 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████▌        | 457/500 [35:21<03:16,  4.57s/it, loss=1.4e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 4.8828125e-07 and disc_lr to 4.8828125e-07 at iteration 457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████▊  | 489/500 [37:47<00:50,  4.64s/it, loss=1.61e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing gen_lr to 2.44140625e-07 and disc_lr to 2.44140625e-07 at iteration 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [38:37<00:00,  4.63s/it, loss=1.71e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- The entire training takes 38.68551256259283 minutes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# hyper-parameter settings\n",
    "# dname = args.dname\n",
    "test = args.test\n",
    "# time_steps = args.time_steps\n",
    "# batch_size = args.batch_size\n",
    "bn = bool(args.bn)\n",
    "# if \"SLURM_ARRAY_TASK_ID\" in os.environ:\n",
    "#     seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "# else:\n",
    "#     seed = args.seed\n",
    "\n",
    "# Dx = args.Dx\n",
    "g_output_activation = 'linear'\n",
    "\n",
    "df = pd.read_csv('./data/spx_20231229.csv', index_col=0, parse_dates=True)\n",
    "if dname == 'AROne':\n",
    "    data_dist = data_utils.AROne(\n",
    "        Dx, time_steps, np.linspace(0.1, 0.9, Dx), 0.5)\n",
    "elif dname == 'eeg':\n",
    "    data_dist = data_utils.EEGData(\n",
    "        Dx, time_steps, batch_size, n_iters, seed=seed)\n",
    "elif dname == 'SineImage':\n",
    "    data_dist = data_utils.SineImage(\n",
    "        length=time_steps, Dx=Dx, rand_std=0.1)\n",
    "elif dname == 'GBM':\n",
    "    data_dist = data_utils.GBM(mu=0.2, sigma=0.5, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                               log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'OU':\n",
    "    data_dist = data_utils.OU(kappa=10., theta=1., sigma=0.5, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                              log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'Heston':\n",
    "    data_dist = data_utils.Heston(mu=0.2, v0=0.25, kappa=1., theta=0.16, rho=-0.7, sigma=0.2, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                                  log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'SPX':\n",
    "    data_dist = data_utils.DFDataset(df, '1995-01-01', '2022-10-19', sample_len, batch_size, stride)\n",
    "elif dname == 'Music':\n",
    "    with open(f'./data/music/melodies_beats_min_5_unique_max_range_24_spec_cluster_12.pkl', 'rb') as f:\n",
    "        songs = pickle.load(f)\n",
    "    cluster_labels = [item[-1] for item in songs]\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    df_clusters = []\n",
    "    for i in range(unique_labels.shape[0]):\n",
    "        df_clusters.append([item for item in songs if item[-1] == i])\n",
    "    cluster = 0\n",
    "    sample_len = 30\n",
    "    batch_size = 32\n",
    "    gap_dur_dpitch_dfs = data_utils.gap_duration_deltapitch_transform([item[0] for item in df_clusters[cluster]])\n",
    "    data_dist = data_utils.GapDurationDeltaPitchDataset(gap_dur_dpitch_dfs, sample_len, batch_size)\n",
    "else:\n",
    "    ValueError('Data does not exist.')\n",
    "\n",
    "dataset = dname\n",
    "# Number of RNN layers stacked together\n",
    "n_layers = 1\n",
    "# reg_penalty = args.reg_penalty\n",
    "# gen_lr = args.lr\n",
    "# disc_lr = args.lr\n",
    "# gen_lr = lr\n",
    "# disc_lr = lr\n",
    "# Add gradient clipping before updates\n",
    "gen_optimiser = tf.keras.optimizers.legacy.Adam(gen_lr)\n",
    "dischm_optimiser = tf.keras.optimizers.legacy.Adam(disc_lr)\n",
    "\n",
    "disc_iters = 1\n",
    "# sinkhorn_eps = args.sinkhorn_eps\n",
    "# sinkhorn_l = args.sinkhorn_l\n",
    "# nlstm = args.nlstm\n",
    "scaling_coef = 1.0\n",
    "\n",
    "# Define a standard multivariate normal for\n",
    "# (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "# z_dims_t = args.z_dims_t\n",
    "if dname == 'SPX':\n",
    "    dist_z = data_utils.GARCH(df, start_date='1995-01-01', end_date='2022-10-19', sample_len=300,\n",
    "                            p=20, o=0, q=0, mean_model='Zero', vol_model='GARCH', dist='gaussian',\n",
    "                            seed=42, stride=50)\n",
    "else:\n",
    "    dist_z = tfp.distributions.Normal(0, 1)\n",
    "    # dist_z = tfp.distributions.Uniform(-1, 1)\n",
    "if not dname in ['GBM', 'OU', 'Heston', 'SPX', 'Music']:\n",
    "    y_dims = args.Dy\n",
    "    dist_y = tfp.distributions.Uniform(-1, 1)\n",
    "\n",
    "# Create instances of generator, discriminator_h and\n",
    "# discriminator_m CONV VERSION\n",
    "# g_state_size = args.g_state_size\n",
    "# d_state_size = args.d_state_size\n",
    "g_filter_size = args.g_filter_size\n",
    "d_filter_size = args.d_filter_size\n",
    "disc_kernel_width = 5\n",
    "\n",
    "if gen_type == \"fc\":\n",
    "    generator = gan.SimpleGenerator(\n",
    "        batch_size, time_steps, Dx, g_filter_size, z_dims_t,\n",
    "        output_activation=g_output_activation)\n",
    "elif gen_type == \"lstm\":\n",
    "    generator = gan.ToyGenerator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, g_state_size, g_filter_size,\n",
    "        output_activation=g_output_activation, nlstm=nlstm, nlayer=2,\n",
    "        Dy=y_dims, bn=bn)\n",
    "elif gen_type == \"genlstm\":\n",
    "    generator = gan.GenLSTM(z_dims_t, Dx, time_steps, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmp\":\n",
    "    generator = gan.GenLSTMp(z_dims_t, Dx, time_steps, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmpdt\":\n",
    "    generator = gan.GenLSTMpdt(z_dims_t, Dx, time_steps, dt, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmd\":\n",
    "    generator = gan.GenLSTMd(z_dims_t, seq_dim, sample_len, hist_len, hidden_size=g_state_size)\n",
    "elif gen_type == 'music':\n",
    "    generator = gan.LSTMusic(z_dims_t, Dx, sample_len, dpitch_range=12)\n",
    "\n",
    "discriminator_h = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "discriminator_m = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "# data_utils.check_model_summary(batch_size, z_dims, generator)\n",
    "# data_utils.check_model_summary(batch_size, seq_len, discriminator_h)\n",
    "\n",
    "# lsinke = int(np.round(np.log10(sinkhorn_eps)))\n",
    "# lreg = int(np.round(np.log10(reg_penalty)))\n",
    "\n",
    "if reg_penalty.is_integer() and sinkhorn_eps.is_integer():\n",
    "    suffix = f\"{dname[:3]}_e{int(sinkhorn_eps):d}r{int(reg_penalty):d}s{seed:d}\"\n",
    "elif reg_penalty.is_integer() and not sinkhorn_eps.is_integer():\n",
    "    suffix = f\"{dname[:3]}_e{sinkhorn_eps:.3g}r{int(reg_penalty):d}s{seed:d}\"\n",
    "elif not reg_penalty.is_integer() and sinkhorn_eps.is_integer():\n",
    "    suffix = f\"{dname[:3]}_e{int(sinkhorn_eps):d}r{reg_penalty:.3g}s{seed:d}\"\n",
    "else:\n",
    "    suffix = f\"{dname[:3]}_e{sinkhorn_eps:.3g}r{reg_penalty:.3g}s{seed:d}\"\n",
    "\n",
    "saved_file =  \"{}_{}{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                    datetime.now().strftime(\"%d\"),\n",
    "                                    datetime.now().strftime(\"%H\"),\n",
    "                                    datetime.now().strftime(\"%M\"),\n",
    "                                    datetime.now().strftime(\"%S\")) + suffix\n",
    "\n",
    "# model_fn = \"%s_Dz%d_Dy%d_Dx%d_bs%d_gss%d_gfs%d_dss%d_dfs%d_ts%d_r%d_eps%d_l%d_lr%d_nl%d_s%02d\" % (\n",
    "#     dname, z_dims_t, y_dims, Dx, batch_size, g_state_size, g_filter_size,\n",
    "#     d_state_size, d_filter_size, time_steps, np.round(np.log10(reg_penalty)),\n",
    "#     np.round(np.log10(sinkhorn_eps)), sinkhorn_l, np.round(np.log10(lr)), nlstm, seed)\n",
    "\n",
    "log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "# Create directories for storing images later.\n",
    "if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/data\")\n",
    "if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "# GAN train notes\n",
    "with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "    # Include any experiment notes here:\n",
    "    f.write(\"Experiment notes: .... \\n\\n\")\n",
    "    f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "        dataset,\n",
    "        time_steps, ))\n",
    "    f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "        g_state_size,\n",
    "        n_layers,\n",
    "        reg_penalty))\n",
    "    f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "        batch_size,\n",
    "        disc_iters,\n",
    "        gen_lr,\n",
    "        disc_lr))\n",
    "    f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "        sinkhorn_eps,\n",
    "        sinkhorn_l))\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "with train_writer.as_default():\n",
    "    tf.summary.text('training_params', data_utils.pretty_json(training_params), step=0)\n",
    "    tf.summary.text('model_params', data_utils.pretty_json(model_params), step=0)\n",
    "    tf.summary.text('data_params', data_utils.pretty_json(data_params), step=0)\n",
    "\n",
    "@tf.function\n",
    "def disc_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as disc_tape:\n",
    "        if dname in ['GBM', 'OU', 'Heston']:\n",
    "            fake_data = generator.call(hidden_z)\n",
    "            fake_data_p = generator.call(hidden_z_p)\n",
    "        elif dname == 'SPX':\n",
    "            fake_data = generator.call(hidden_z, real_data)\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p)\n",
    "        elif dname == 'Music':\n",
    "            fake_data = generator.call(hidden_z, real_data[:, :hist_len, :], real_data[:, hist_len:, :2])\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p[:, :hist_len, :], real_data_p[:, hist_len:, :2])\n",
    "            real_pitch = tf.cumsum(real_data[:,:,-1:], axis=1)\n",
    "            real_pitch_p = tf.cumsum(real_data_p[:,:,-1:], axis=1)\n",
    "            real_data = tf.concat([real_data[:,:,:2], real_pitch], axis=-1)\n",
    "            real_data_p = tf.concat([real_data_p[:,:,:2], real_pitch_p], axis=-1)\n",
    "        else:\n",
    "            hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "            hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "            fake_data = generator.call(hidden_z, hidden_y)\n",
    "            fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "\n",
    "        # h_fake = discriminator_h.call(fake_data)\n",
    "        # m_real = discriminator_m.call(real_data)\n",
    "        # m_fake = discriminator_m.call(fake_data)\n",
    "        # h_real_p = discriminator_h.call(real_data_p)\n",
    "        # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "        # m_real_p = discriminator_m.call(real_data_p)\n",
    "        # loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "        #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "        #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "        #     h_real_p, h_fake_p)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "        # NOTE: FOR USING hist_len ONWARDS FOR LOSS COMPUTATION\n",
    "        h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "        m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "        m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "        h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "        h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "        m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # print(f'fake_data shape: {fake_data[:,hist_len:,:].shape}')\n",
    "        # print(f'fake_data_p shape: {fake_data_p[:,hist_len:,:].shape}')\n",
    "        # print(f'real_data shape: {real_data[:,hist_len:,:].shape}')\n",
    "        # print(f'real_data_p shape: {real_data_p[:,hist_len:,:].shape}')\n",
    "        # print(f'm_real shape: {m_real.shape}')\n",
    "        # print(f'm_fake shape: {m_fake.shape}')\n",
    "        # print(f'h_fake shape: {h_fake.shape}')\n",
    "        # print(f'm_real_p shape: {m_real_p.shape}')\n",
    "        # print(f'h_real_p shape: {h_real_p.shape}')\n",
    "        # print(f'h_fake_p shape: {h_fake_p.shape}')\n",
    "        loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "        pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "            m_real, reg_penalty, scaling_coef)\n",
    "        disc_loss = - loss1 + pm1\n",
    "    # update discriminator parameters\n",
    "    disch_grads, discm_grads = disc_tape.gradient(\n",
    "        disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "    dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "    dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def gen_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        if dname in ['GBM', 'OU', 'Heston']:\n",
    "            fake_data = generator.call(hidden_z)\n",
    "            fake_data_p = generator.call(hidden_z_p)\n",
    "        elif dname == 'SPX':\n",
    "            fake_data = generator.call(hidden_z, real_data)\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p)\n",
    "        elif dname == 'Music':\n",
    "            fake_data = generator.call(hidden_z, real_data[:, :hist_len, :], real_data[:, hist_len:, :2])\n",
    "            fake_data_p = generator.call(hidden_z_p, real_data_p[:, :hist_len, :], real_data_p[:, hist_len:, :2])\n",
    "            real_pitch = tf.cumsum(real_data[:,:,-1:], axis=1)\n",
    "            real_pitch_p = tf.cumsum(real_data_p[:,:,-1:], axis=1)\n",
    "            real_data = tf.concat([real_data[:,:,:2], real_pitch], axis=-1)\n",
    "            real_data_p = tf.concat([real_data_p[:,:,:2], real_pitch_p], axis=-1)\n",
    "        else:\n",
    "            hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "            hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "            fake_data = generator.call(hidden_z, hidden_y)\n",
    "            fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "\n",
    "        # h and m networks used to compute the martingale penalty\n",
    "\n",
    "        # h_fake = discriminator_h.call(fake_data)\n",
    "        # m_real = discriminator_m.call(real_data)\n",
    "        # m_fake = discriminator_m.call(fake_data)\n",
    "        # h_real_p = discriminator_h.call(real_data_p)\n",
    "        # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "        # m_real_p = discriminator_m.call(real_data_p)\n",
    "        # loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "        #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "        #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "        #     h_real_p, h_fake_p)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "        # # NOTE: FOR USING hist_len ONWARDS FOR LOSS COMPUTATION\n",
    "        h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "        m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "        m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "        h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "        h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "        m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "        loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "        gen_loss = loss2\n",
    "    # update generator parameters\n",
    "    generator_grads = gen_tape.gradient(\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "    return loss2\n",
    "\n",
    "it_counts = 0\n",
    "with tqdm.trange(n_iters, ncols=150) as it:\n",
    "    best_loss = [np.inf, 0]\n",
    "    for _ in it:\n",
    "        it_counts += 1\n",
    "        # generate a batch of REAL data\n",
    "        real_data = data_dist.batch(batch_size)\n",
    "        real_data_p = data_dist.batch(batch_size)\n",
    "        real_data = tf.cast(real_data, tf.float32)\n",
    "        real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "        disc_training_step(real_data, real_data_p)\n",
    "        loss = gen_training_step(real_data, real_data_p)\n",
    "        it.set_postfix(loss=float(loss))\n",
    "\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "            train_writer.flush()\n",
    "\n",
    "        if not np.isfinite(loss.numpy()):\n",
    "            # print('%s Loss exploded!' % model_fn)\n",
    "            print('Loss exploded')\n",
    "            # Open the existing file with mode a - append\n",
    "            with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                # Include any experiment notes here:\n",
    "                f.write(\"\\n Training failed! \")\n",
    "            break\n",
    "        else:\n",
    "            # check if the loss is the best so far and reduce lr if no improvement beyond patience\n",
    "            if loss < best_loss[0]:\n",
    "                best_loss = [loss, it_counts]\n",
    "            if it_counts - best_loss[1] > patience:\n",
    "                gen_lr *= factor\n",
    "                disc_lr *= factor\n",
    "                gen_optimiser.lr.assign(gen_lr)\n",
    "                dischm_optimiser.lr.assign(disc_lr)\n",
    "                best_loss = [loss, it_counts] # reset best loss iteration to current iteration for next patience\n",
    "                print(f'Reducing gen_lr to {gen_lr} and disc_lr to {disc_lr} at iteration {it_counts}')\n",
    "\n",
    "            # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "            z = dist_z.sample([batch_size, sample_len-1, z_dims_t])\n",
    "            if dname in ['GBM', 'OU', 'Heston']:\n",
    "                samples = generator.call(z, training=False)\n",
    "            elif dname == 'SPX':\n",
    "                samples = generator.call(z, real_data, training=False) # For SPX\n",
    "            elif dname == 'Music':\n",
    "                samples = generator.call(z, real_data[:, :hist_len, :], real_data[:, hist_len:, :2], training=False)\n",
    "                real_pitch = tf.cumsum(real_data[:,:,-1:], axis=1)\n",
    "                real_data = tf.concat([real_data[:,:,:2], real_pitch], axis=-1)\n",
    "            else:\n",
    "                y = dist_y.sample([batch_size, y_dims])\n",
    "                samples = generator.call(z, y, training=False)\n",
    "\n",
    "            batch_series = np.asarray(samples[...,1])\n",
    "            if log_series:\n",
    "                plt.plot(np.exp(batch_series.T))\n",
    "                sample_mean = np.diff(batch_series, axis=1).mean() / dt\n",
    "                sample_std = np.diff(batch_series, axis=1).std() / np.sqrt(dt)\n",
    "            else:\n",
    "                plt.plot(batch_series.T)\n",
    "                sample_mean = np.diff(np.log(batch_series), axis=1).mean() / dt\n",
    "                sample_std = np.diff(np.log(batch_series), axis=1).std() / np.sqrt(dt)\n",
    "            # save plot to file\n",
    "            # if samples.shape[-1] == 1:\n",
    "            #     data_utils.plot_batch(np.asarray(samples[..., 0]), it_counts, saved_file)\n",
    "\n",
    "            # img = tf.transpose(tf.concat(list(samples), axis=1))[None, :, :, None]\n",
    "            with train_writer.as_default():\n",
    "                if it_counts % fig_freq == 0:\n",
    "                    tf.summary.image(\"Generated samples\", data_utils.plot_to_image(plt.gcf()), step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_mean', sample_mean, step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_std', sample_std, step=it_counts)\n",
    "            # save model to file\n",
    "            generator.save_weights(f\"./trained/{saved_file}/generator/\")\n",
    "            discriminator_h.save_weights(f\"./trained/{saved_file}/discriminator_h/\")\n",
    "            discriminator_m.save_weights(f\"./trained/{saved_file}/discriminator_m/\")\n",
    "        continue\n",
    "\n",
    "print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
