{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "import data_utils\n",
    "import gan_utils\n",
    "import gan\n",
    "\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"4\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"4\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "batch_size = 64\n",
    "sinkhorn_eps = 100 # entropy regularisation coefficent (will take log10 of this number and round to int)\n",
    "sinkhorn_l = 100 # number of sinkhorn iterations\n",
    "reg_penalty = 100 # martingale regularisation penalty (will take log10 of this number and round to int)\n",
    "lr = 1e-3\n",
    "\n",
    "gen_type = 'lstmd'\n",
    "activation = 'tanh'\n",
    "nlstm = 1\n",
    "g_state_size = 64\n",
    "d_state_size = 64\n",
    "log_series = True\n",
    "\n",
    "dname = 'SPX'\n",
    "seq_dim = 1 # dimension of the time series excluding time dimension\n",
    "Dx = 2 # dimension of the time series including time dimension\n",
    "time_steps = 250\n",
    "sample_len = 300\n",
    "hist_len = 50\n",
    "stride = 50\n",
    "seed = 42 # np.random.randint(0, 10000)\n",
    "dt = 1 / 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_iters': n_iters,\n",
    "    'batch_size': batch_size,\n",
    "    'sinkhorn_eps': sinkhorn_eps,\n",
    "    'sinkhorn_l': sinkhorn_l,\n",
    "    'reg_penalty': reg_penalty,\n",
    "    'lr': lr,\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'gen_type': gen_type,\n",
    "    'activation': activation,\n",
    "    'nlstm': nlstm,\n",
    "    'g_state_size': g_state_size,\n",
    "    'd_state_size': d_state_size,\n",
    "    'log_series': log_series,\n",
    "}\n",
    "\n",
    "data_params = {\n",
    "    'dname': dname,\n",
    "    'dt': dt,\n",
    "    'time_steps': time_steps,\n",
    "    'seed': seed,\n",
    "    'Dx': Dx,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='cot')\n",
    "\n",
    "# parser.add_argument('-d', '--dname', type=str, default='GBM',\n",
    "#                     choices=['SineImage', 'AROne', 'eeg', 'GBM'])\n",
    "parser.add_argument('-t', '--test', type=str, default='cot',\n",
    "                    choices=['cot'])\n",
    "# parser.add_argument('-s', '--seed', type=int, default=42)\n",
    "# parser.add_argument('-gss', '--g_state_size', type=int, default=32)\n",
    "# parser.add_argument('-dss', '--d_state_size', type=int, default=32)\n",
    "parser.add_argument('-gfs', '--g_filter_size', type=int, default=32)\n",
    "parser.add_argument('-dfs', '--d_filter_size', type=int, default=32)\n",
    "# parser.add_argument('-r', '--reg_penalty', type=float, default=10.0) # martingale regularisation coefficent\n",
    "# parser.add_argument('-ts', '--time_steps', type=int, default=60)\n",
    "# parser.add_argument('-sinke', '--sinkhorn_eps', type=float, default=100) # entropy regularisation coefficent\n",
    "# parser.add_argument('-sinkl', '--sinkhorn_l', type=int, default=100) # number of sinkhorn iterations\n",
    "# parser.add_argument('-Dx', '--Dx', type=int, default=1)\n",
    "parser.add_argument('-Dy', '--Dy', type=int, default=10)\n",
    "parser.add_argument('-Dz', '--z_dims_t', type=int, default=4)\n",
    "# parser.add_argument('-g', '--gen', type=str, default=\"genlstm\",\n",
    "#                     choices=[\"lstm\", \"fc\", \"genlstm\"])\n",
    "# parser.add_argument('-bs', '--batch_size', type=int, default=64)\n",
    "# parser.add_argument('-nlstm', '--nlstm', type=int, default=1,\n",
    "                    # help=\"number of lstms in discriminator\")\n",
    "# parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
    "parser.add_argument('-bn', '--bn', type=int, default=1,\n",
    "                    help=\"batch norm\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 5855.988337142852\n",
      "            Iterations: 37\n",
      "            Function evaluations: 859\n",
      "            Gradient evaluations: 37\n",
      "                        Zero Mean - ARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:           gaussianized   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                       ARCH   Log-Likelihood:               -5855.99\n",
      "Distribution:                  Normal   AIC:                           11754.0\n",
      "Method:            Maximum Likelihood   BIC:                           11897.9\n",
      "                                        No. Observations:                 6999\n",
      "Date:                Tue, Apr 02 2024   Df Residuals:                     6999\n",
      "Time:                        17:40:10   Df Model:                            0\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega          0.0727  8.049e-03      9.032  1.688e-19  [5.692e-02,8.848e-02]\n",
      "alpha[1]       0.0186  1.285e-02      1.449      0.147 [-6.566e-03,4.380e-02]\n",
      "alpha[2]       0.0827  1.292e-02      6.401  1.546e-10    [5.736e-02,  0.108]\n",
      "alpha[3]       0.0258  1.273e-02      2.028  4.254e-02  [8.689e-04,5.075e-02]\n",
      "alpha[4]       0.0722  1.427e-02      5.061  4.163e-07    [4.426e-02,  0.100]\n",
      "alpha[5]       0.1752  1.634e-02     10.721  8.127e-27      [  0.143,  0.207]\n",
      "alpha[6]       0.0267  1.268e-02      2.101  3.562e-02  [1.792e-03,5.151e-02]\n",
      "alpha[7]       0.0282  1.428e-02      1.978  4.795e-02  [2.544e-04,5.621e-02]\n",
      "alpha[8]       0.0191  1.346e-02      1.419      0.156 [-7.277e-03,4.547e-02]\n",
      "alpha[9]       0.0556  1.477e-02      3.764  1.673e-04  [2.664e-02,8.452e-02]\n",
      "alpha[10]      0.1145  1.513e-02      7.571  3.714e-14    [8.487e-02,  0.144]\n",
      "alpha[11]      0.0196  1.386e-02      1.414      0.157 [-7.571e-03,4.676e-02]\n",
      "alpha[12]  4.4795e-03  1.334e-02      0.336      0.737 [-2.166e-02,3.062e-02]\n",
      "alpha[13]  8.0320e-12  1.147e-02  7.002e-10      1.000 [-2.248e-02,2.248e-02]\n",
      "alpha[14]      0.0266  1.424e-02      1.868  6.179e-02 [-1.312e-03,5.449e-02]\n",
      "alpha[15]      0.0318  1.366e-02      2.329  1.986e-02  [5.041e-03,5.858e-02]\n",
      "alpha[16]  7.9793e-12  1.425e-02  5.599e-10      1.000 [-2.793e-02,2.793e-02]\n",
      "alpha[17]  9.4517e-12  1.227e-02  7.703e-10      1.000 [-2.405e-02,2.405e-02]\n",
      "alpha[18]  1.2805e-11  1.414e-02  9.055e-10      1.000 [-2.771e-02,2.771e-02]\n",
      "alpha[19]      0.0806  1.513e-02      5.331  9.787e-08    [5.099e-02,  0.110]\n",
      "alpha[20]      0.0205  1.290e-02      1.588      0.112 [-4.793e-03,4.577e-02]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 17:40:10.374262: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-04-02 17:40:10.374285: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-02 17:40:10.374294: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-02 17:40:10.374325: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-02 17:40:10.374339: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "  0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake_data shape: (64, 250, 2)\n",
      "fake_data_p shape: (64, 250, 2)\n",
      "real_data shape: (64, 250, 2)\n",
      "real_data_p shape: (64, 250, 2)\n",
      "m_real shape: (64, 250, 64)\n",
      "m_fake shape: (64, 250, 64)\n",
      "h_fake shape: (64, 250, 64)\n",
      "m_real_p shape: (64, 250, 64)\n",
      "h_real_p shape: (64, 250, 64)\n",
      "h_fake_p shape: (64, 250, 64)\n",
      "fake_data shape: (64, 250, 2)\n",
      "fake_data_p shape: (64, 250, 2)\n",
      "real_data shape: (64, 250, 2)\n",
      "real_data_p shape: (64, 250, 2)\n",
      "m_real shape: (64, 250, 64)\n",
      "m_fake shape: (64, 250, 64)\n",
      "h_fake shape: (64, 250, 64)\n",
      "m_real_p shape: (64, 250, 64)\n",
      "h_real_p shape: (64, 250, 64)\n",
      "h_fake_p shape: (64, 250, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 17:44:32.908860: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# hyper-parameter settings\n",
    "# dname = args.dname\n",
    "test = args.test\n",
    "# time_steps = args.time_steps\n",
    "# batch_size = args.batch_size\n",
    "bn = bool(args.bn)\n",
    "# if \"SLURM_ARRAY_TASK_ID\" in os.environ:\n",
    "#     seed = int(os.environ[\"SLURM_ARRAY_TASK_ID\"])\n",
    "# else:\n",
    "#     seed = args.seed\n",
    "\n",
    "# Dx = args.Dx\n",
    "g_output_activation = 'linear'\n",
    "\n",
    "df = pd.read_csv('./data/spx_20231229.csv', index_col=0, parse_dates=True)\n",
    "if dname == 'AROne':\n",
    "    data_dist = data_utils.AROne(\n",
    "        Dx, time_steps, np.linspace(0.1, 0.9, Dx), 0.5)\n",
    "elif dname == 'eeg':\n",
    "    data_dist = data_utils.EEGData(\n",
    "        Dx, time_steps, batch_size, n_iters, seed=seed)\n",
    "elif dname == 'SineImage':\n",
    "    data_dist = data_utils.SineImage(\n",
    "        length=time_steps, Dx=Dx, rand_std=0.1)\n",
    "elif dname == 'GBM':\n",
    "    data_dist = data_utils.GBM(mu=0.2, sigma=0.5, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                               log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'OU':\n",
    "    data_dist = data_utils.OU(kappa=10., theta=1., sigma=0.5, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                              log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'Heston':\n",
    "    data_dist = data_utils.Heston(mu=0.2, v0=0.25, kappa=1., theta=0.16, rho=-0.7, sigma=0.2, dt=dt, length=time_steps, batch_size=batch_size, n_paths=batch_size*100,\n",
    "                                  log_series=log_series, initial_value=1.0, time_dim=False, seed=seed)\n",
    "elif dname == 'SPX':\n",
    "    data_dist = data_utils.DFDataset(df, '1995-01-01', '2022-10-19', sample_len, batch_size, stride)\n",
    "else:\n",
    "    ValueError('Data does not exist.')\n",
    "\n",
    "dataset = dname\n",
    "# Number of RNN layers stacked together\n",
    "n_layers = 1\n",
    "# reg_penalty = args.reg_penalty\n",
    "# gen_lr = args.lr\n",
    "# disc_lr = args.lr\n",
    "gen_lr = lr\n",
    "disc_lr = lr\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# Add gradient clipping before updates\n",
    "gen_optimiser = tf.keras.optimizers.legacy.Adam(gen_lr)\n",
    "dischm_optimiser = tf.keras.optimizers.legacy.Adam(disc_lr)\n",
    "\n",
    "it_counts = 0\n",
    "disc_iters = 1\n",
    "# sinkhorn_eps = args.sinkhorn_eps\n",
    "# sinkhorn_l = args.sinkhorn_l\n",
    "# nlstm = args.nlstm\n",
    "scaling_coef = 1.0\n",
    "\n",
    "# Define a standard multivariate normal for\n",
    "# (z1, z2, ..., zT) --> (y1, y2, ..., yT)\n",
    "z_dims_t = args.z_dims_t\n",
    "y_dims = args.Dy\n",
    "# dist_z = tfp.distributions.Uniform(-1, 1)\n",
    "# dist_z = tfp.distributions.Normal(0, 1)\n",
    "dist_z = data_utils.GARCH(df, start_date='1995-01-01', end_date='2022-10-19', sample_len=300,\n",
    "                          p=20, o=0, q=0, mean_model='Zero', vol_model='GARCH', dist='gaussian',\n",
    "                          seed=42, stride=50)\n",
    "# dist_y = tfp.distributions.Uniform(-1, 1)\n",
    "\n",
    "# Create instances of generator, discriminator_h and\n",
    "# discriminator_m CONV VERSION\n",
    "# g_state_size = args.g_state_size\n",
    "# d_state_size = args.d_state_size\n",
    "g_filter_size = args.g_filter_size\n",
    "d_filter_size = args.d_filter_size\n",
    "disc_kernel_width = 5\n",
    "\n",
    "if gen_type == \"fc\":\n",
    "    generator = gan.SimpleGenerator(\n",
    "        batch_size, time_steps, Dx, g_filter_size, z_dims_t,\n",
    "        output_activation=g_output_activation)\n",
    "elif gen_type == \"lstm\":\n",
    "    generator = gan.ToyGenerator(\n",
    "        batch_size, time_steps, z_dims_t, Dx, g_state_size, g_filter_size,\n",
    "        output_activation=g_output_activation, nlstm=nlstm, nlayer=2,\n",
    "        Dy=y_dims, bn=bn)\n",
    "elif gen_type == \"genlstm\":\n",
    "    generator = gan.GenLSTM(z_dims_t, Dx, time_steps, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmp\":\n",
    "    generator = gan.GenLSTMp(z_dims_t, Dx, time_steps, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmpdt\":\n",
    "    generator = gan.GenLSTMpdt(z_dims_t, Dx, time_steps, dt, hidden_size=g_state_size, activation=activation, n_lstm_layers=nlstm, log_series=log_series)\n",
    "elif gen_type == \"lstmd\":\n",
    "    generator = gan.GenLSTMd(z_dims_t, seq_dim, sample_len, hist_len, hidden_size=g_state_size)\n",
    "\n",
    "discriminator_h = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "discriminator_m = gan.ToyDiscriminator(\n",
    "    batch_size, time_steps, z_dims_t, Dx, d_state_size, d_filter_size,\n",
    "    kernel_size=disc_kernel_width, nlayer=2, nlstm=0, bn=bn)\n",
    "\n",
    "# data_utils.check_model_summary(batch_size, z_dims, generator)\n",
    "# data_utils.check_model_summary(batch_size, seq_len, discriminator_h)\n",
    "\n",
    "lsinke = int(np.round(np.log10(sinkhorn_eps)))\n",
    "lreg = int(np.round(np.log10(reg_penalty)))\n",
    "saved_file = f\"{dname[:3]}_{test[0]}_e{lsinke:d}r{lreg:d}s{seed:d}\" + \\\n",
    "    \"{}_{}{}-{}-{}-{}-{}\".format(dataset, datetime.now().strftime(\"%h\"),\n",
    "                                    datetime.now().strftime(\"%d\"),\n",
    "                                    datetime.now().strftime(\"%H\"),\n",
    "                                    datetime.now().strftime(\"%M\"),\n",
    "                                    datetime.now().strftime(\"%S\"),\n",
    "                                    datetime.now().strftime(\"%f\"))\n",
    "\n",
    "model_fn = \"%s_Dz%d_Dy%d_Dx%d_bs%d_gss%d_gfs%d_dss%d_dfs%d_ts%d_r%d_eps%d_l%d_lr%d_nl%d_s%02d\" % (\n",
    "    dname, z_dims_t, y_dims, Dx, batch_size, g_state_size, g_filter_size,\n",
    "    d_state_size, d_filter_size, time_steps, np.round(np.log10(reg_penalty)),\n",
    "    np.round(np.log10(sinkhorn_eps)), sinkhorn_l, np.round(np.log10(lr)), nlstm, seed)\n",
    "\n",
    "log_dir = f\"./trained/{saved_file}/log\"\n",
    "\n",
    "# Create directories for storing images later.\n",
    "if not os.path.exists(f\"trained/{saved_file}/data\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/data\")\n",
    "if not os.path.exists(f\"trained/{saved_file}/images\"):\n",
    "    os.makedirs(f\"trained/{saved_file}/images\")\n",
    "\n",
    "# GAN train notes\n",
    "with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
    "    # Include any experiment notes here:\n",
    "    f.write(\"Experiment notes: .... \\n\\n\")\n",
    "    f.write(\"MODEL_DATA: {}\\nSEQ_LEN: {}\\n\".format(\n",
    "        dataset,\n",
    "        time_steps, ))\n",
    "    f.write(\"STATE_SIZE: {}\\nNUM_LAYERS: {}\\nLAMBDA: {}\\n\".format(\n",
    "        g_state_size,\n",
    "        n_layers,\n",
    "        reg_penalty))\n",
    "    f.write(\"BATCH_SIZE: {}\\nCRITIC_ITERS: {}\\nGenerator LR: {}\\nDiscriminator LR:{}\\n\".format(\n",
    "        batch_size,\n",
    "        disc_iters,\n",
    "        gen_lr,\n",
    "        disc_lr))\n",
    "    f.write(\"SINKHORN EPS: {}\\nSINKHORN L: {}\\n\\n\".format(\n",
    "        sinkhorn_eps,\n",
    "        sinkhorn_l))\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "with train_writer.as_default():\n",
    "    tf.summary.text('training_params', data_utils.pretty_json(training_params), step=0)\n",
    "    tf.summary.text('model_params', data_utils.pretty_json(model_params), step=0)\n",
    "    tf.summary.text('data_params', data_utils.pretty_json(data_params), step=0)\n",
    "\n",
    "@tf.function\n",
    "def disc_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as disc_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        # fake_data = generator.call(hidden_z)                  # For GBM, OU, Heston\n",
    "        # fake_data_p = generator.call(hidden_z_p)              # For GBM, OU, Heston\n",
    "        fake_data = generator.call(hidden_z, real_data)         # For SPX\n",
    "        fake_data_p = generator.call(hidden_z_p, real_data_p)   # For SPX\n",
    "\n",
    "        # h_fake = discriminator_h.call(fake_data)\n",
    "        h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # m_real = discriminator_m.call(real_data)\n",
    "        m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "        # m_fake = discriminator_m.call(fake_data)\n",
    "        m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "        # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # m_real_p = discriminator_m.call(real_data_p)\n",
    "        m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "        #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "        #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "        #     h_real_p, h_fake_p)\n",
    "        # print(f'fake_data shape: {fake_data[:,hist_len:,:].shape}')\n",
    "        # print(f'fake_data_p shape: {fake_data_p[:,hist_len:,:].shape}')\n",
    "        # print(f'real_data shape: {real_data[:,hist_len:,:].shape}')\n",
    "        # print(f'real_data_p shape: {real_data_p[:,hist_len:,:].shape}')\n",
    "        # print(f'm_real shape: {m_real.shape}')\n",
    "        # print(f'm_fake shape: {m_fake.shape}')\n",
    "        # print(f'h_fake shape: {h_fake.shape}')\n",
    "        # print(f'm_real_p shape: {m_real_p.shape}')\n",
    "        # print(f'h_real_p shape: {h_real_p.shape}')\n",
    "        # print(f'h_fake_p shape: {h_fake_p.shape}')\n",
    "        loss1 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        pm1 = gan_utils.scale_invariante_martingale_regularization(\n",
    "            m_real, reg_penalty, scaling_coef)\n",
    "        disc_loss = - loss1 + pm1\n",
    "    # update discriminator parameters\n",
    "    disch_grads, discm_grads = disc_tape.gradient(\n",
    "        disc_loss, [discriminator_h.trainable_variables, discriminator_m.trainable_variables])\n",
    "    dischm_optimiser.apply_gradients(zip(disch_grads, discriminator_h.trainable_variables))\n",
    "    dischm_optimiser.apply_gradients(zip(discm_grads, discriminator_m.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def gen_training_step(real_data, real_data_p):\n",
    "    hidden_z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    hidden_z_p = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "    # hidden_y = dist_y.sample([batch_size, y_dims])\n",
    "    # hidden_y_p = dist_y.sample([batch_size, y_dims])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # fake_data = generator.call(hidden_z, hidden_y)\n",
    "        # fake_data_p = generator.call(hidden_z_p, hidden_y_p)\n",
    "        # fake_data = generator.call(hidden_z)                      # For GBM, OU, Heston\n",
    "        # fake_data_p = generator.call(hidden_z_p)                  # For GBM, OU, Heston\n",
    "        fake_data = generator.call(hidden_z, real_data)             # For SPX\n",
    "        fake_data_p = generator.call(hidden_z_p, real_data_p)       # For SPX\n",
    "\n",
    "        # h and m networks used to compute the martingale penalty\n",
    "\n",
    "        # h_fake = discriminator_h.call(fake_data)\n",
    "        h_fake = discriminator_h.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # m_real = discriminator_m.call(real_data)\n",
    "        m_real = discriminator_m.call(real_data[:,hist_len:,:]) # For SPX\n",
    "        # m_fake = discriminator_m.call(fake_data)\n",
    "        m_fake = discriminator_m.call(fake_data[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # h_real_p = discriminator_h.call(real_data_p)\n",
    "        h_real_p = discriminator_h.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "        # h_fake_p = discriminator_h.call(fake_data_p)\n",
    "        h_fake_p = discriminator_h.call(fake_data_p[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # m_real_p = discriminator_m.call(real_data_p)\n",
    "        m_real_p = discriminator_m.call(real_data_p[:,hist_len:,:]) # For SPX\n",
    "\n",
    "        # loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "        #     real_data, fake_data, m_real, m_fake, h_fake, scaling_coef,\n",
    "        #     sinkhorn_eps, sinkhorn_l, real_data_p, fake_data_p, m_real_p,\n",
    "        #     h_real_p, h_fake_p)\n",
    "        loss2 = gan_utils.compute_mixed_sinkhorn_loss(\n",
    "            real_data[:,hist_len:,:], fake_data[:,hist_len:,:], m_real, m_fake, h_fake, scaling_coef,\n",
    "            sinkhorn_eps, sinkhorn_l, real_data_p[:,hist_len:,:], fake_data_p[:,hist_len:,:], m_real_p,\n",
    "            h_real_p, h_fake_p)\n",
    "        gen_loss = loss2\n",
    "    # update generator parameters\n",
    "    generator_grads = gen_tape.gradient(\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gen_optimiser.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
    "    return loss2\n",
    "\n",
    "with tqdm.trange(n_iters, ncols=150) as it:\n",
    "    for _ in it:\n",
    "        it_counts += 1\n",
    "        # generate a batch of REAL data\n",
    "        real_data = data_dist.batch(batch_size)\n",
    "        real_data_p = data_dist.batch(batch_size)\n",
    "        real_data = tf.cast(real_data, tf.float32)\n",
    "        real_data_p = tf.cast(real_data_p, tf.float32)\n",
    "\n",
    "        disc_training_step(real_data, real_data_p)\n",
    "        loss = gen_training_step(real_data, real_data_p)\n",
    "        it.set_postfix(loss=float(loss))\n",
    "\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('Sinkhorn loss', loss, step=it_counts)\n",
    "            train_writer.flush()\n",
    "\n",
    "        if not np.isfinite(loss.numpy()):\n",
    "            print('%s Loss exploded!' % model_fn)\n",
    "            # Open the existing file with mode a - append\n",
    "            with open(\"./trained/{}/train_notes.txt\".format(saved_file), 'a') as f:\n",
    "                # Include any experiment notes here:\n",
    "                f.write(\"\\n Training failed! \")\n",
    "            break\n",
    "        else:\n",
    "            # print(\"Plot samples produced by generator after %d iterations\" % it_counts)\n",
    "            z = dist_z.sample([batch_size, time_steps, z_dims_t])\n",
    "            # y = dist_y.sample([batch_size, y_dims])\n",
    "            # samples = generator.call(z, y, training=False)\n",
    "            samples = generator.call(z, training=False)\n",
    "            batch_series = np.asarray(samples[..., 1:])\n",
    "            if log_series:\n",
    "                plt.plot(np.exp(batch_series.T))\n",
    "                sample_mean = np.diff(batch_series, axis=1).mean() / dt\n",
    "                sample_std = np.diff(batch_series, axis=1).std() / np.sqrt(dt)\n",
    "            else:\n",
    "                plt.plot(batch_series.T)\n",
    "                sample_mean = np.diff(np.log(batch_series), axis=1).mean() / dt\n",
    "                sample_std = np.diff(np.log(batch_series), axis=1).std() / np.sqrt(dt)\n",
    "            # save plot to file\n",
    "            # if samples.shape[-1] == 1:\n",
    "            #     data_utils.plot_batch(np.asarray(samples[..., 0]), it_counts, saved_file)\n",
    "\n",
    "            # img = tf.transpose(tf.concat(list(samples), axis=1))[None, :, :, None]\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.image(\"Generated samples\", data_utils.plot_to_image(plt.gcf()), step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_mean', sample_mean, step=it_counts)\n",
    "                tf.summary.scalar('Stats/Sample_std', sample_std, step=it_counts)\n",
    "            # save model to file\n",
    "            generator.save_weights(f\"./trained/{saved_file}/generator/\")\n",
    "            discriminator_h.save_weights(f\"./trained/{saved_file}/discriminator_h/\")\n",
    "            discriminator_m.save_weights(f\"./trained/{saved_file}/discriminator_m/\")\n",
    "        continue\n",
    "\n",
    "print(\"--- The entire training takes %s minutes ---\" % ((time.time() - start_time) / 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
